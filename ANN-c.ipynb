{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    '''\n",
    "    MNIST data set \n",
    "    x has 784 feature\n",
    "    y is op value from 0 to 9 \n",
    "    '''\n",
    "    data = np.array(pd.read_csv('data/MNIST/MNIST_train.csv'))\n",
    "    x = (data[:,1:]/255.).T\n",
    "    y = oneHot(data[:,0])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def ReLU(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return Z > 0\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            op = Activation.sigmoid(Z)\n",
    "            return op * (1- op)\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z,isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 1\n",
    "        Z = Z - np.max(Z, axis=0)\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    @staticmethod\n",
    "    def multiClass(target, prediction):\n",
    "        return np.argmax(target, axis=0) == np.argmax(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error:\n",
    "    @staticmethod\n",
    "    def meanSquareError(target, predicted, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 2 * (predicted - target) / np.size(target)\n",
    "        loss = np.power(target - predicted, 2)\n",
    "        accuracy = Accuracy.multiClass(target, predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(target, predicted,  isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return predicted - target\n",
    "        loss = -target * np.log(predicted + 10 ** -100)\n",
    "        accuracy = Accuracy.multiClass(target,predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def hiddenError(target, predicted):\n",
    "        return target.T.dot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    op = []\n",
    "    bias = []\n",
    "    weight = []\n",
    "    output = []\n",
    "    isInput=False\n",
    "    noOfNodes = 0\n",
    "    activation = None\n",
    "    def __init__(self, inputSize=0, outputSize=0, activation=None,isInput=False,input=[]):\n",
    "        '''\n",
    "        inputSize -> no.of.input feature \n",
    "        outputSize -> no.of.output\n",
    "        '''\n",
    "        if isInput:\n",
    "            self.output = input\n",
    "            self.isInput = True\n",
    "            self.noOfNodes = input.shape[0]\n",
    "        else :\n",
    "            self.noOfNodes = inputSize\n",
    "            self.activation = activation\n",
    "            self.weight = np.random.rand(inputSize, outputSize) - 0.5\n",
    "            self.bias = np.random.rand(inputSize,1) - 0.5\n",
    "\n",
    "    def generateWeight(self,*r):\n",
    "        '''\n",
    "        Receive input as set that define the set shape\n",
    "        '''\n",
    "        return np.random.randn(*r) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    input = []\n",
    "    target = []\n",
    "    layers = []\n",
    "    history = {'loss': [], 'accuracy':[]}\n",
    "    loss = None\n",
    "\n",
    "    @staticmethod\n",
    "    def forWord(w, b, x):\n",
    "        return np.dot(w, x) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(w, b, x, err, lr=0.01):\n",
    "        w = w - (1/len(x[0]) * (err.dot(x.T))) * lr\n",
    "        b = b - (lr * np.mean(err, axis=1).reshape(b.shape))\n",
    "        return w,b  \n",
    "\n",
    "    def __init__(self, input, target, loss):\n",
    "        self.loss = loss\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.layers.append(\n",
    "            DenseLayer(None,None,None,isInput=True, input=self.input)\n",
    "        )\n",
    "\n",
    "    def append(self, node, activationFunction):\n",
    "        preNode = self.layers[len(self.layers) -1]\n",
    "        self.layers.append(\n",
    "            DenseLayer(inputSize=node,outputSize=preNode.noOfNodes, activation=activationFunction)\n",
    "        )\n",
    "\n",
    "    def train(self, epoch=100, lr=0.01):\n",
    "        for i in range(epoch):\n",
    "\n",
    "            for j in range(len(self.layers)):\n",
    "\n",
    "                if not self.layers[j].isInput:\n",
    "                    self.layers[j].op = NeuralNetwork.forWord(self.layers[j].weight, self.layers[j].bias, self.layers[j-1].output)\n",
    "                    self.layers[j].output = self.layers[j].activation(self.layers[j].op)\n",
    "\n",
    "                if j == len(self.layers)-1 :\n",
    "                    loss = self.loss(self.target,self.layers[j].output)\n",
    "                    self.history['loss'].append(np.mean(loss['loss']))\n",
    "                    self.history['accuracy'].append(np.mean(loss['accuracy']))\n",
    "                    print(f\"Epoch: {i+1} Loss: {self.history['loss'][-1]} Accuracy: {self.history['accuracy'][-1]}\")\n",
    "                    # calculate loss\n",
    "                    self.backPropogation(loss['loss'],lr)\n",
    "\n",
    "\n",
    "    def backPropogation(self, loss,lr):\n",
    "        layer_length = len(self.layers)\n",
    "        for index, layer in enumerate(self.layers[::-1]):\n",
    "            if not layer.isInput:\n",
    "                pervious_node = self.layers[layer_length - index - 2]\n",
    "\n",
    "                layer.weight, layer.bias = NeuralNetwork.gradient(\n",
    "                    w=layer.weight, b=layer.bias, x=pervious_node.output, err=loss,lr=lr)\n",
    "                    \n",
    "                oldw = np.copy(layer.weight)\n",
    "\n",
    "                loss = Error.hiddenError(oldw, loss)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer :  1 9.695222881785638 0.0\n",
      "Layer :  2 0.8758029390824879 4.5009360749428186e-06\n",
      "Epoch: 1 Loss: 0.2840997948748314 Accuracy: 0.08061904761904762\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.691253650337572 0.0\n",
      "Layer :  2 0.8783575991244833 4.327687858149742e-06\n",
      "Epoch: 2 Loss: 0.285423818488201 Accuracy: 0.08028571428571428\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.696799199368703 0.0\n",
      "Layer :  2 0.8872659220361144 3.9217769070783815e-06\n",
      "Epoch: 3 Loss: 0.286973191037366 Accuracy: 0.07980952380952382\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.710681650097998 0.0\n",
      "Layer :  2 0.8958455142728211 3.492410184188924e-06\n",
      "Epoch: 4 Loss: 0.2887759890707759 Accuracy: 0.08021428571428571\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.733333253383385 0.0\n",
      "Layer :  2 0.9039815809889961 3.0474377473614095e-06\n",
      "Epoch: 5 Loss: 0.2908687868943239 Accuracy: 0.08033333333333334\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.765292301616135 0.0\n",
      "Layer :  2 0.909089675084675 2.5969294149774826e-06\n",
      "Epoch: 6 Loss: 0.29327078808788115 Accuracy: 0.08061904761904762\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.807219961695028 0.0\n",
      "Layer :  2 0.9143202073501778 2.1530680753140863e-06\n",
      "Epoch: 7 Loss: 0.29602704974607513 Accuracy: 0.08085714285714286\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.859924719892437 0.0\n",
      "Layer :  2 0.9196651078183846 1.7290838254922023e-06\n",
      "Epoch: 8 Loss: 0.29917991698551044 Accuracy: 0.08064285714285714\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.924391121043312 0.0\n",
      "Layer :  2 0.925111979735681 1.3380484688137289e-06\n",
      "Epoch: 9 Loss: 0.30279877207770584 Accuracy: 0.08154761904761905\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 10.001822992480927 0.0\n",
      "Layer :  2 0.930644995277718 9.915404661253634e-07\n",
      "Epoch: 10 Loss: 0.30697305234954886 Accuracy: 0.08221428571428571\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(np.copy(x),np.copy(y),Error.crossEntropyLoss)\n",
    "\n",
    "model.append(10, Activation.ReLU)\n",
    "model.append(10,Activation.softmax)\n",
    "model.train(10)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.09957427570808244 Accuracy: 0.10680952380952381\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,10) (784,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m, Activation\u001b[39m.\u001b[39mReLU)\n\u001b[1;32m      4\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m,Activation\u001b[39m.\u001b[39msoftmax)\n\u001b[0;32m----> 5\u001b[0m model1\u001b[39m.\u001b[39;49mtrain(\u001b[39m10\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39mdel\u001b[39;00m model1\n",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, epoch, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m Accuracy: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39m# calculate loss\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackPropogation(loss[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m],lr)\n",
      "Cell \u001b[0;32mIn[8], line 58\u001b[0m, in \u001b[0;36mNeuralNetwork.backPropogation\u001b[0;34m(self, loss, lr)\u001b[0m\n\u001b[1;32m     55\u001b[0m pervious_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[layer_length \u001b[39m-\u001b[39m index \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Layer Weight : \u001b[39m\u001b[39m\"\u001b[39m,layer\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape, \u001b[39m\"\u001b[39m\u001b[39m Pre layer Weight : \u001b[39m\u001b[39m\"\u001b[39m,pervious_node\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pervious_node\u001b[39m.\u001b[39mweight) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m layer\u001b[39m.\u001b[39mweight, layer\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m NeuralNetwork\u001b[39m.\u001b[39;49mgradient(\n\u001b[1;32m     59\u001b[0m     w\u001b[39m=\u001b[39;49mlayer\u001b[39m.\u001b[39;49mweight, b\u001b[39m=\u001b[39;49mlayer\u001b[39m.\u001b[39;49mbias, x\u001b[39m=\u001b[39;49mpervious_node\u001b[39m.\u001b[39;49moutput, err\u001b[39m=\u001b[39;49mloss,lr\u001b[39m=\u001b[39;49mlr)\n\u001b[1;32m     61\u001b[0m oldw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(layer\u001b[39m.\u001b[39mweight)\n\u001b[1;32m     63\u001b[0m loss \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39mhiddenError(oldw, loss)\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mNeuralNetwork.gradient\u001b[0;34m(w, b, x, err, lr)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(w, b, x, err, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39;49m (\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39mlen\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]) \u001b[39m*\u001b[39;49m (err\u001b[39m.\u001b[39;49mdot(x\u001b[39m.\u001b[39;49mT))) \u001b[39m*\u001b[39;49m lr\n\u001b[1;32m     15\u001b[0m     b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m (lr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(err, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(b\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m w,b\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,10) (784,10) "
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetwork(np.copy(x),np.copy(y),Error.meanSquareError)\n",
    "\n",
    "model1.append(10, Activation.ReLU)\n",
    "model1.append(10,Activation.softmax)\n",
    "model1.train(10, 0.1)\n",
    "del model1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(epoch, loss, lr=0.1):\n",
    "    hw = np.random.rand(10, 784) - 0.5\n",
    "    hb = np.random.rand(10, 1) - 0.5\n",
    "    ow = np.random.rand(10, 10) - 0.5\n",
    "    ob = np.random.rand(10, 1) - 0.5\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        #forword\n",
    "        #Hidden\n",
    "        hid_op = NeuralNetwork.forWord(hw,hb,x)\n",
    "        hid_act = Activation.ReLU(hid_op)\n",
    "\n",
    "        #op layer\n",
    "        op = NeuralNetwork.forWord(ow,ob,hid_act)\n",
    "        y_pred = Activation.softmax(op)\n",
    "\n",
    "        # backword\n",
    "        # output error\n",
    "        op_err = loss(y, y_pred, True) * Activation.softmax(op,True)\n",
    "        ow,ob = NeuralNetwork.gradient(w=ow, b=ob, x=hid_act,err=op_err,lr=lr)\n",
    "\n",
    "        # #hidden error\n",
    "        hid_err = Error.hiddenError(ow,op_err) * Activation.ReLU(hid_op,True)\n",
    "        hw,hb = NeuralNetwork.gradient(w=hw, b=hb, x=x,err=hid_err, lr=lr)\n",
    "\n",
    "        err = loss(y, y_pred)\n",
    "        print(f\"Epoch : {i + 1}, Loss : {np.mean(err['loss'])}, Accuracy : {np.mean(err['accuracy'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.34088263436142524, Accuracy : 0.12157142857142857\n",
      "Epoch : 2, Loss : 0.28871713950843847, Accuracy : 0.14923809523809523\n",
      "Epoch : 3, Loss : 0.26792198899743125, Accuracy : 0.15895238095238096\n",
      "Epoch : 4, Loss : 0.2551714713739749, Accuracy : 0.16523809523809524\n",
      "Epoch : 5, Loss : 0.24659889125042836, Accuracy : 0.17254761904761906\n",
      "Epoch : 6, Loss : 0.24043032279098517, Accuracy : 0.18183333333333335\n",
      "Epoch : 7, Loss : 0.23556972321796343, Accuracy : 0.1925952380952381\n",
      "Epoch : 8, Loss : 0.23140001645167174, Accuracy : 0.20711904761904762\n",
      "Epoch : 9, Loss : 0.2276349588699246, Accuracy : 0.21942857142857142\n",
      "Epoch : 10, Loss : 0.22415270565376905, Accuracy : 0.2318095238095238\n",
      "Epoch : 11, Loss : 0.22088336052424143, Accuracy : 0.24457142857142858\n",
      "Epoch : 12, Loss : 0.21779004106003602, Accuracy : 0.2566428571428571\n",
      "Epoch : 13, Loss : 0.2148453765714435, Accuracy : 0.2670952380952381\n",
      "Epoch : 14, Loss : 0.21202673923744203, Accuracy : 0.27735714285714286\n",
      "Epoch : 15, Loss : 0.2093145270557699, Accuracy : 0.28745238095238096\n",
      "Epoch : 16, Loss : 0.20669677109580173, Accuracy : 0.29738095238095236\n",
      "Epoch : 17, Loss : 0.20416227368388393, Accuracy : 0.30647619047619046\n",
      "Epoch : 18, Loss : 0.20170310097330812, Accuracy : 0.31652380952380954\n",
      "Epoch : 19, Loss : 0.1993110626830072, Accuracy : 0.32452380952380955\n",
      "Epoch : 20, Loss : 0.19698061070990244, Accuracy : 0.3341190476190476\n",
      "Epoch : 21, Loss : 0.19470556779467532, Accuracy : 0.3420952380952381\n",
      "Epoch : 22, Loss : 0.19248114569634825, Accuracy : 0.3501190476190476\n",
      "Epoch : 23, Loss : 0.19030968283527455, Accuracy : 0.35773809523809524\n",
      "Epoch : 24, Loss : 0.18818650527142786, Accuracy : 0.3656190476190476\n",
      "Epoch : 25, Loss : 0.18610767996680808, Accuracy : 0.37314285714285716\n",
      "Epoch : 26, Loss : 0.1840718808014557, Accuracy : 0.3796428571428571\n",
      "Epoch : 27, Loss : 0.18207984470202562, Accuracy : 0.3858809523809524\n",
      "Epoch : 28, Loss : 0.18012976674623588, Accuracy : 0.3918333333333333\n",
      "Epoch : 29, Loss : 0.17822134988263214, Accuracy : 0.3978333333333333\n",
      "Epoch : 30, Loss : 0.17635048762694697, Accuracy : 0.40335714285714286\n",
      "Epoch : 31, Loss : 0.17451674115856833, Accuracy : 0.4092142857142857\n",
      "Epoch : 32, Loss : 0.1727194356279904, Accuracy : 0.4145952380952381\n",
      "Epoch : 33, Loss : 0.17095784385509477, Accuracy : 0.42083333333333334\n",
      "Epoch : 34, Loss : 0.1692304353575216, Accuracy : 0.4258571428571429\n",
      "Epoch : 35, Loss : 0.16753451010935605, Accuracy : 0.43204761904761907\n",
      "Epoch : 36, Loss : 0.16587240925548574, Accuracy : 0.43785714285714283\n",
      "Epoch : 37, Loss : 0.16424462111801386, Accuracy : 0.44316666666666665\n",
      "Epoch : 38, Loss : 0.16264754700990355, Accuracy : 0.4476428571428571\n",
      "Epoch : 39, Loss : 0.16108199063503287, Accuracy : 0.45254761904761903\n",
      "Epoch : 40, Loss : 0.1595462127470282, Accuracy : 0.4569761904761905\n",
      "Epoch : 41, Loss : 0.15803803157593443, Accuracy : 0.46126190476190476\n",
      "Epoch : 42, Loss : 0.15655682195250453, Accuracy : 0.4650714285714286\n",
      "Epoch : 43, Loss : 0.1551037940415981, Accuracy : 0.4699761904761905\n",
      "Epoch : 44, Loss : 0.1536763053308421, Accuracy : 0.4745238095238095\n",
      "Epoch : 45, Loss : 0.15227301910504, Accuracy : 0.47883333333333333\n",
      "Epoch : 46, Loss : 0.1508951085487505, Accuracy : 0.4826666666666667\n",
      "Epoch : 47, Loss : 0.14954220145373362, Accuracy : 0.4871190476190476\n",
      "Epoch : 48, Loss : 0.14821262262869087, Accuracy : 0.49152380952380953\n",
      "Epoch : 49, Loss : 0.14690521220446326, Accuracy : 0.4953095238095238\n",
      "Epoch : 50, Loss : 0.14561934164618295, Accuracy : 0.49976190476190474\n",
      "Epoch : 51, Loss : 0.14435443554651756, Accuracy : 0.5039761904761905\n",
      "Epoch : 52, Loss : 0.14310905599241036, Accuracy : 0.5077619047619047\n",
      "Epoch : 53, Loss : 0.14188126120682987, Accuracy : 0.5115\n",
      "Epoch : 54, Loss : 0.14067185871481924, Accuracy : 0.5154047619047619\n",
      "Epoch : 55, Loss : 0.13948082395158864, Accuracy : 0.5191904761904762\n",
      "Epoch : 56, Loss : 0.13830822642217097, Accuracy : 0.5231428571428571\n",
      "Epoch : 57, Loss : 0.13715443718685988, Accuracy : 0.5266190476190477\n",
      "Epoch : 58, Loss : 0.1360179069322285, Accuracy : 0.5307857142857143\n",
      "Epoch : 59, Loss : 0.13489647899441481, Accuracy : 0.5353095238095238\n",
      "Epoch : 60, Loss : 0.13378933373034627, Accuracy : 0.5394285714285715\n",
      "Epoch : 61, Loss : 0.1326985111997092, Accuracy : 0.5434285714285715\n",
      "Epoch : 62, Loss : 0.13162254851618585, Accuracy : 0.5470238095238096\n",
      "Epoch : 63, Loss : 0.1305602579335026, Accuracy : 0.5509285714285714\n",
      "Epoch : 64, Loss : 0.12951003743479153, Accuracy : 0.5541428571428572\n",
      "Epoch : 65, Loss : 0.12847326657870262, Accuracy : 0.5579047619047619\n",
      "Epoch : 66, Loss : 0.12744857216529545, Accuracy : 0.5613095238095238\n",
      "Epoch : 67, Loss : 0.12643676445054228, Accuracy : 0.5648333333333333\n",
      "Epoch : 68, Loss : 0.12543907646933844, Accuracy : 0.5693095238095238\n",
      "Epoch : 69, Loss : 0.12445411893890197, Accuracy : 0.5732619047619048\n",
      "Epoch : 70, Loss : 0.12348418625234231, Accuracy : 0.5772619047619048\n",
      "Epoch : 71, Loss : 0.12252979181286344, Accuracy : 0.5809761904761904\n",
      "Epoch : 72, Loss : 0.12158816484723187, Accuracy : 0.5846666666666667\n",
      "Epoch : 73, Loss : 0.12065764910855846, Accuracy : 0.588\n",
      "Epoch : 74, Loss : 0.1197391100170953, Accuracy : 0.5915714285714285\n",
      "Epoch : 75, Loss : 0.11883334892703579, Accuracy : 0.5950714285714286\n",
      "Epoch : 76, Loss : 0.11794051719361878, Accuracy : 0.599\n",
      "Epoch : 77, Loss : 0.11706011183644212, Accuracy : 0.602547619047619\n",
      "Epoch : 78, Loss : 0.11619362066529387, Accuracy : 0.605904761904762\n",
      "Epoch : 79, Loss : 0.11534088691131868, Accuracy : 0.6091190476190477\n",
      "Epoch : 80, Loss : 0.11450140978114416, Accuracy : 0.6119285714285714\n",
      "Epoch : 81, Loss : 0.11367402297102902, Accuracy : 0.6152857142857143\n",
      "Epoch : 82, Loss : 0.1128610706898844, Accuracy : 0.6181190476190476\n",
      "Epoch : 83, Loss : 0.11206052569518812, Accuracy : 0.6215238095238095\n",
      "Epoch : 84, Loss : 0.11127342928640292, Accuracy : 0.6243571428571428\n",
      "Epoch : 85, Loss : 0.11050098739827079, Accuracy : 0.6274285714285714\n",
      "Epoch : 86, Loss : 0.1097434863513977, Accuracy : 0.6306428571428572\n",
      "Epoch : 87, Loss : 0.10900027022864181, Accuracy : 0.6336666666666667\n",
      "Epoch : 88, Loss : 0.10827068866350467, Accuracy : 0.6367142857142857\n",
      "Epoch : 89, Loss : 0.1075563651877007, Accuracy : 0.6390238095238095\n",
      "Epoch : 90, Loss : 0.10685540738772646, Accuracy : 0.6416904761904761\n",
      "Epoch : 91, Loss : 0.10616807259539068, Accuracy : 0.6444761904761904\n",
      "Epoch : 92, Loss : 0.10549369303033806, Accuracy : 0.647\n",
      "Epoch : 93, Loss : 0.10483249553871167, Accuracy : 0.6498095238095238\n",
      "Epoch : 94, Loss : 0.10418396634547777, Accuracy : 0.6524047619047619\n",
      "Epoch : 95, Loss : 0.1035482069861765, Accuracy : 0.6548809523809523\n",
      "Epoch : 96, Loss : 0.10292526432723523, Accuracy : 0.6568333333333334\n",
      "Epoch : 97, Loss : 0.10231359822380849, Accuracy : 0.6593333333333333\n",
      "Epoch : 98, Loss : 0.10171309217697386, Accuracy : 0.662\n",
      "Epoch : 99, Loss : 0.10112437035387105, Accuracy : 0.6640952380952381\n",
      "Epoch : 100, Loss : 0.10054707006725716, Accuracy : 0.6668333333333333\n"
     ]
    }
   ],
   "source": [
    "debug(100,Error.crossEntropyLoss,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.11869653794026085, Accuracy : 0.0884047619047619\n",
      "Epoch : 2, Loss : 0.11869644640788884, Accuracy : 0.0884047619047619\n",
      "Epoch : 3, Loss : 0.11869635487570838, Accuracy : 0.0884047619047619\n",
      "Epoch : 4, Loss : 0.11869626334406605, Accuracy : 0.0884047619047619\n",
      "Epoch : 5, Loss : 0.11869617181296181, Accuracy : 0.0884047619047619\n",
      "Epoch : 6, Loss : 0.1186960802823957, Accuracy : 0.0884047619047619\n",
      "Epoch : 7, Loss : 0.11869598875236766, Accuracy : 0.0884047619047619\n",
      "Epoch : 8, Loss : 0.11869589722287772, Accuracy : 0.0884047619047619\n",
      "Epoch : 9, Loss : 0.11869580569392589, Accuracy : 0.0884047619047619\n",
      "Epoch : 10, Loss : 0.11869571416551218, Accuracy : 0.0884047619047619\n",
      "Epoch : 11, Loss : 0.11869562263773854, Accuracy : 0.0884047619047619\n",
      "Epoch : 12, Loss : 0.11869553111050966, Accuracy : 0.0884047619047619\n",
      "Epoch : 13, Loss : 0.1186954395838189, Accuracy : 0.0884047619047619\n",
      "Epoch : 14, Loss : 0.11869534805766617, Accuracy : 0.0884047619047619\n",
      "Epoch : 15, Loss : 0.1186952565320516, Accuracy : 0.0884047619047619\n",
      "Epoch : 16, Loss : 0.11869516500707974, Accuracy : 0.0884047619047619\n",
      "Epoch : 17, Loss : 0.1186950734828378, Accuracy : 0.0884047619047619\n",
      "Epoch : 18, Loss : 0.11869498195913392, Accuracy : 0.0884047619047619\n",
      "Epoch : 19, Loss : 0.11869489043596806, Accuracy : 0.0884047619047619\n",
      "Epoch : 20, Loss : 0.11869479891334034, Accuracy : 0.0884047619047619\n",
      "Epoch : 21, Loss : 0.11869470739125051, Accuracy : 0.0884047619047619\n",
      "Epoch : 22, Loss : 0.11869461586969884, Accuracy : 0.0884047619047619\n",
      "Epoch : 23, Loss : 0.11869452434868517, Accuracy : 0.0884047619047619\n",
      "Epoch : 24, Loss : 0.11869443282820953, Accuracy : 0.0884047619047619\n",
      "Epoch : 25, Loss : 0.11869434130827194, Accuracy : 0.0884047619047619\n",
      "Epoch : 26, Loss : 0.11869424978887236, Accuracy : 0.0884047619047619\n",
      "Epoch : 27, Loss : 0.11869415826896075, Accuracy : 0.0884047619047619\n",
      "Epoch : 28, Loss : 0.11869406674910701, Accuracy : 0.0884047619047619\n",
      "Epoch : 29, Loss : 0.11869397522979137, Accuracy : 0.0884047619047619\n",
      "Epoch : 30, Loss : 0.11869388371101366, Accuracy : 0.0884047619047619\n",
      "Epoch : 31, Loss : 0.11869379219279073, Accuracy : 0.0884047619047619\n",
      "Epoch : 32, Loss : 0.11869370067534242, Accuracy : 0.0884047619047619\n",
      "Epoch : 33, Loss : 0.11869360915843206, Accuracy : 0.0884047619047619\n",
      "Epoch : 34, Loss : 0.11869351764205978, Accuracy : 0.0884047619047619\n",
      "Epoch : 35, Loss : 0.1186934261262254, Accuracy : 0.0884047619047619\n",
      "Epoch : 36, Loss : 0.118693334610929, Accuracy : 0.0884047619047619\n",
      "Epoch : 37, Loss : 0.11869324309617467, Accuracy : 0.0884047619047619\n",
      "Epoch : 38, Loss : 0.11869315158224335, Accuracy : 0.0884047619047619\n",
      "Epoch : 39, Loss : 0.11869306006884994, Accuracy : 0.0884047619047619\n",
      "Epoch : 40, Loss : 0.11869296855599454, Accuracy : 0.0884047619047619\n",
      "Epoch : 41, Loss : 0.1186928770436771, Accuracy : 0.0884047619047619\n",
      "Epoch : 42, Loss : 0.11869278553189765, Accuracy : 0.0884047619047619\n",
      "Epoch : 43, Loss : 0.11869269402065614, Accuracy : 0.0884047619047619\n",
      "Epoch : 44, Loss : 0.11869260250995256, Accuracy : 0.0884047619047619\n",
      "Epoch : 45, Loss : 0.11869251099978693, Accuracy : 0.0884047619047619\n",
      "Epoch : 46, Loss : 0.11869241949015927, Accuracy : 0.0884047619047619\n",
      "Epoch : 47, Loss : 0.11869232798106948, Accuracy : 0.0884047619047619\n",
      "Epoch : 48, Loss : 0.11869223647251771, Accuracy : 0.0884047619047619\n",
      "Epoch : 49, Loss : 0.11869214496450384, Accuracy : 0.0884047619047619\n",
      "Epoch : 50, Loss : 0.11869205345703215, Accuracy : 0.0884047619047619\n",
      "Epoch : 51, Loss : 0.1186919619503829, Accuracy : 0.0884047619047619\n",
      "Epoch : 52, Loss : 0.1186918704442714, Accuracy : 0.0884047619047619\n",
      "Epoch : 53, Loss : 0.11869177893869795, Accuracy : 0.0884047619047619\n",
      "Epoch : 54, Loss : 0.11869168743366236, Accuracy : 0.0884047619047619\n",
      "Epoch : 55, Loss : 0.11869159592916467, Accuracy : 0.0884047619047619\n",
      "Epoch : 56, Loss : 0.11869150442469614, Accuracy : 0.0884047619047619\n",
      "Epoch : 57, Loss : 0.11869141292014594, Accuracy : 0.0884047619047619\n",
      "Epoch : 58, Loss : 0.11869132141613369, Accuracy : 0.0884047619047619\n",
      "Epoch : 59, Loss : 0.11869122991265928, Accuracy : 0.0884047619047619\n",
      "Epoch : 60, Loss : 0.11869113840972285, Accuracy : 0.0884047619047619\n",
      "Epoch : 61, Loss : 0.11869104690732425, Accuracy : 0.0884047619047619\n",
      "Epoch : 62, Loss : 0.11869095540546357, Accuracy : 0.0884047619047619\n",
      "Epoch : 63, Loss : 0.1186908639041348, Accuracy : 0.0884047619047619\n",
      "Epoch : 64, Loss : 0.11869077240314752, Accuracy : 0.0884047619047619\n",
      "Epoch : 65, Loss : 0.11869068090283755, Accuracy : 0.0884047619047619\n",
      "Epoch : 66, Loss : 0.118690589403523, Accuracy : 0.0884047619047619\n",
      "Epoch : 67, Loss : 0.11869049790468235, Accuracy : 0.0884047619047619\n",
      "Epoch : 68, Loss : 0.1186904064063424, Accuracy : 0.0884047619047619\n",
      "Epoch : 69, Loss : 0.11869031490854035, Accuracy : 0.0884047619047619\n",
      "Epoch : 70, Loss : 0.11869022341127612, Accuracy : 0.0884047619047619\n",
      "Epoch : 71, Loss : 0.11869013191454975, Accuracy : 0.0884047619047619\n",
      "Epoch : 72, Loss : 0.11869004041836119, Accuracy : 0.08838095238095238\n",
      "Epoch : 73, Loss : 0.11868994892271048, Accuracy : 0.08838095238095238\n",
      "Epoch : 74, Loss : 0.11868985742759758, Accuracy : 0.08838095238095238\n",
      "Epoch : 75, Loss : 0.11868976593302255, Accuracy : 0.08838095238095238\n",
      "Epoch : 76, Loss : 0.1186896744392116, Accuracy : 0.08838095238095238\n",
      "Epoch : 77, Loss : 0.11868958294719696, Accuracy : 0.08838095238095238\n",
      "Epoch : 78, Loss : 0.11868949145572011, Accuracy : 0.08838095238095238\n",
      "Epoch : 79, Loss : 0.11868939996478103, Accuracy : 0.08838095238095238\n",
      "Epoch : 80, Loss : 0.11868930847437982, Accuracy : 0.08838095238095238\n",
      "Epoch : 81, Loss : 0.11868921698451633, Accuracy : 0.08838095238095238\n",
      "Epoch : 82, Loss : 0.11868912549519069, Accuracy : 0.08838095238095238\n",
      "Epoch : 83, Loss : 0.11868903400640285, Accuracy : 0.08838095238095238\n",
      "Epoch : 84, Loss : 0.11868894251815282, Accuracy : 0.08838095238095238\n",
      "Epoch : 85, Loss : 0.11868885103044054, Accuracy : 0.08838095238095238\n",
      "Epoch : 86, Loss : 0.11868875954326599, Accuracy : 0.08838095238095238\n",
      "Epoch : 87, Loss : 0.11868866805662932, Accuracy : 0.08838095238095238\n",
      "Epoch : 88, Loss : 0.1186885765705304, Accuracy : 0.08838095238095238\n",
      "Epoch : 89, Loss : 0.11868848508496925, Accuracy : 0.08838095238095238\n",
      "Epoch : 90, Loss : 0.11868839360062027, Accuracy : 0.08838095238095238\n",
      "Epoch : 91, Loss : 0.11868830211746428, Accuracy : 0.08838095238095238\n",
      "Epoch : 92, Loss : 0.11868821063484598, Accuracy : 0.08838095238095238\n",
      "Epoch : 93, Loss : 0.11868811915276548, Accuracy : 0.08838095238095238\n",
      "Epoch : 94, Loss : 0.11868802767122277, Accuracy : 0.08838095238095238\n",
      "Epoch : 95, Loss : 0.11868793619021768, Accuracy : 0.08838095238095238\n",
      "Epoch : 96, Loss : 0.11868784470975043, Accuracy : 0.08838095238095238\n",
      "Epoch : 97, Loss : 0.11868775322982095, Accuracy : 0.08838095238095238\n",
      "Epoch : 98, Loss : 0.11868766175042912, Accuracy : 0.08838095238095238\n",
      "Epoch : 99, Loss : 0.11868757027157505, Accuracy : 0.08838095238095238\n",
      "Epoch : 100, Loss : 0.11868747879325874, Accuracy : 0.08838095238095238\n"
     ]
    }
   ],
   "source": [
    "debug(100,Error.meanSquareError,0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
