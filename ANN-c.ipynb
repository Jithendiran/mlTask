{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    '''\n",
    "    MNIST data set \n",
    "    x has 784 feature\n",
    "    y is op value from 0 to 9 \n",
    "    '''\n",
    "    data = np.array(pd.read_csv('data/MNIST/MNIST_train.csv'))\n",
    "    x = (data[:,1:]/255).T\n",
    "    y = oneHot(data[:,0])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def ReLU(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return Z > 0\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            op = Activation.sigmoid(Z)\n",
    "            return op * (1- op)\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z,isDerivation=False):\n",
    "        if isDerivation:\n",
    "            pass\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    @staticmethod\n",
    "    def multiClass(target, prediction):\n",
    "        return np.argmax(target, axis=0) == np.argmax(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error:\n",
    "    def meanSquareError(self, isDerivation=False):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(target, predicted,  isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return predicted - target\n",
    "        loss = -target * np.log(predicted)\n",
    "        accuracy = Accuracy.multiClass(target,predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def hiddenError(target, predicted):\n",
    "        return target.T.dot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    bias = []\n",
    "    weight = []\n",
    "    output = []\n",
    "    isInput=False\n",
    "    noOfNodes = 0\n",
    "    activation = None\n",
    "    def __init__(self, inputSize=0, outputSize=0, activation=None,isInput=False,input=[]):\n",
    "        '''\n",
    "        inputSize -> no.of.input feature \n",
    "        outputSize -> no.of.output\n",
    "        '''\n",
    "        if isInput:\n",
    "            self.output = input\n",
    "            self.isInput = True\n",
    "            self.noOfNodes = input.shape[0]\n",
    "        else :\n",
    "            self.noOfNodes = inputSize\n",
    "            self.activation = activation\n",
    "            self.weight = self.generateWeight(inputSize, outputSize)\n",
    "            self.bias = self.generateWeight(inputSize,1)\n",
    "\n",
    "    def generateWeight(self,*r):\n",
    "        '''\n",
    "        Receive input as set that define the set shape\n",
    "        '''\n",
    "        return np.random.randn(*r) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    input = []\n",
    "    target = []\n",
    "    layers = []\n",
    "    history = {'loss': [], 'accuracy':[]}\n",
    "    loss = None\n",
    "\n",
    "    def __init__(self, input, target, loss):\n",
    "        self.loss = loss\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.layers.append(DenseLayer(None,None,None,isInput=True, input=self.input))\n",
    "\n",
    "    def append(self, node, activationFunction):\n",
    "        preNode = self.layers[len(self.layers) -1]\n",
    "        self.layers.append(DenseLayer(inputSize=node,outputSize=preNode.noOfNodes, activation=activationFunction))\n",
    "\n",
    "    def train(self, epoch=100):\n",
    "        for i in range(epoch):\n",
    "            for j in range(len(self.layers)):\n",
    "                if not self.layers[j].isInput:\n",
    "                    self.layers[j].output = self.forWord(self.layers[j], self.layers[j-1].output)\n",
    "                if j == len(self.layers)-1 :\n",
    "                    loss = self.loss(self.target,self.layers[j].output)\n",
    "                    self.history['loss'].append(np.mean(loss['loss']))\n",
    "                    self.history['accuracy'].append(np.mean(loss['accuracy']))\n",
    "                    index = len(self.history['loss']) -1\n",
    "                    print(f\"Epoch: {i+1} Loss: {self.history['loss'][index]} Accuracy: {self.history['accuracy'][index]}\")\n",
    "                    # calculate loss\n",
    "                    self.backPropogation(loss['loss'])\n",
    "\n",
    "\n",
    "\n",
    "    def forWord(self, layer, input):\n",
    "        return layer.activation(np.dot(layer.weight, input) + layer.bias)\n",
    "\n",
    "\n",
    "    def backPropogation(self, loss):\n",
    "        layer_length = len(self.layers)\n",
    "        for index,layer in enumerate(self.layers[::-1]):\n",
    "            if not layer.isInput:\n",
    "                pervious_node = self.layers[layer_length - index - 1]\n",
    "                if index != 0:\n",
    "                    # previous layer error\n",
    "                    loss = Error.hiddenError(pervious_node.weight,loss)\n",
    "                layer.weight,layer.bias = self.gradient(layer.weight, layer.bias, pervious_node.output, loss)\n",
    "\n",
    "\n",
    "    def gradient(self, w, b, x, err, lr=0.01):\n",
    "        w = w - (1/len(x[0]) * (err.dot(x.T))) * lr\n",
    "        b = b - (lr * np.mean(err, axis=1).reshape(b.shape))\n",
    "        return w,b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = NeuralNetwork(x,y,Error.crossEntropyLoss)\n",
    "\n",
    "model.append(10, Activation.ReLU)\n",
    "model.append(10,Activation.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.2833525205223664 Accuracy: 0.09971428571428571\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,784) (784,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain(\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][index]\u001b[39m}\u001b[39;00m\u001b[39m Accuracy: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][index]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# calculate loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackPropogation(loss[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m, in \u001b[0;36mNeuralNetwork.backPropogation\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[39m# previous layer error\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     loss \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39mhiddenError(pervious_node\u001b[39m.\u001b[39mweight,loss)\n\u001b[0;32m---> 47\u001b[0m layer\u001b[39m.\u001b[39mweight,layer\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(layer\u001b[39m.\u001b[39;49mweight, layer\u001b[39m.\u001b[39;49mbias, x, loss)\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mNeuralNetwork.gradient\u001b[0;34m(self, w, b, x, err, lr)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(\u001b[39mself\u001b[39m, w, b, x, err, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39;49m (\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39mlen\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]) \u001b[39m*\u001b[39;49m (err\u001b[39m.\u001b[39;49mdot(x\u001b[39m.\u001b[39;49mT))) \u001b[39m*\u001b[39;49m lr\n\u001b[1;32m     52\u001b[0m     b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m (lr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(err, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(b\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m w,b\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,784) (784,10) "
     ]
    }
   ],
   "source": [
    "model.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 42000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.copy(model.layers[2].output)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.      , 2.242442, 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-y[:,0]*np.log(test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:,0]\n",
    "np.log(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09945238095238096"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(y,axis=0) == np.argmax(test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2415347100265878"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(-y * np.log(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.DenseLayer at 0x7f6582f1fee0>,\n",
       " <__main__.DenseLayer at 0x7f6582f1fa30>,\n",
       " <__main__.DenseLayer at 0x7f6582f1fc10>,\n",
       " <__main__.DenseLayer at 0x7f6582e124a0>,\n",
       " <__main__.DenseLayer at 0x7f6582aab9d0>,\n",
       " <__main__.DenseLayer at 0x7f65b4926770>,\n",
       " <__main__.DenseLayer at 0x7f6582d3e410>,\n",
       " <__main__.DenseLayer at 0x7f6582d3e0e0>,\n",
       " <__main__.DenseLayer at 0x7f6582ae92d0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
