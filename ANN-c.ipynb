{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    '''\n",
    "    MNIST data set \n",
    "    x has 784 feature\n",
    "    y is op value from 0 to 9 \n",
    "    '''\n",
    "    data = np.array(pd.read_csv('data/MNIST/MNIST_train.csv'))\n",
    "    x = (data[:,1:]/255).T\n",
    "    y = oneHot(data[:,0])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def ReLU(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return Z > 0\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            op = Activation.sigmoid(Z)\n",
    "            return op * (1- op)\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z,isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 1\n",
    "        Z = Z - np.max(Z, axis=0)\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    @staticmethod\n",
    "    def multiClass(target, prediction):\n",
    "        return np.argmax(target, axis=0) == np.argmax(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error:\n",
    "    @staticmethod\n",
    "    def meanSquareError(target, predicted, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 2 * (predicted - target) / np.size(target)\n",
    "        loss = np.power(target - predicted, 2)\n",
    "        accuracy = Accuracy.multiClass(target, predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(target, predicted,  isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return predicted - target\n",
    "        loss = -target * np.log(predicted + 10 ** -100)\n",
    "        accuracy = Accuracy.multiClass(target,predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def hiddenError(target, predicted):\n",
    "        return target.T.dot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    op = []\n",
    "    bias = []\n",
    "    weight = []\n",
    "    output = []\n",
    "    isInput=False\n",
    "    noOfNodes = 0\n",
    "    activation = None\n",
    "    def __init__(self, inputSize=0, outputSize=0, activation=None,isInput=False,input=[]):\n",
    "        '''\n",
    "        inputSize -> no.of.input feature \n",
    "        outputSize -> no.of.output\n",
    "        '''\n",
    "        if isInput:\n",
    "            self.output = input\n",
    "            self.isInput = True\n",
    "            self.noOfNodes = input.shape[0]\n",
    "        else :\n",
    "            self.noOfNodes = inputSize\n",
    "            self.activation = activation\n",
    "            self.weight = np.random.rand(inputSize, outputSize) - 0.5\n",
    "            self.bias = np.random.rand(inputSize,1) - 0.5\n",
    "\n",
    "    def generateWeight(self,*r):\n",
    "        '''\n",
    "        Receive input as set that define the set shape\n",
    "        '''\n",
    "        return np.random.randn(*r) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    input = []\n",
    "    target = []\n",
    "    layers = []\n",
    "    history = {'loss': [], 'accuracy':[]}\n",
    "    loss = None\n",
    "\n",
    "    def __init__(self, input, target, loss):\n",
    "        self.loss = loss\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.layers.append(DenseLayer(None,None,None,isInput=True, input=self.input))\n",
    "\n",
    "    def append(self, node, activationFunction):\n",
    "        preNode = self.layers[len(self.layers) -1]\n",
    "        self.layers.append(DenseLayer(inputSize=node,outputSize=preNode.noOfNodes, activation=activationFunction))\n",
    "\n",
    "    def train(self, epoch=100, lr=0.01):\n",
    "        for i in range(epoch):\n",
    "            for j in range(len(self.layers)):\n",
    "                if not self.layers[j].isInput:\n",
    "                    self.layers[j].op = self.forWord(self.layers[j], self.layers[j-1].output)\n",
    "                    self.layers[j].output = self.layers[j].activation(self.layers[j].op)\n",
    "                if j == len(self.layers)-1 :\n",
    "                    loss = self.loss(self.target,self.layers[j].output)\n",
    "                    self.history['loss'].append(np.mean(loss['loss']))\n",
    "                    self.history['accuracy'].append(np.mean(loss['accuracy']))\n",
    "                    # index = len(self.history['loss']) -1\n",
    "                    print(f\"Epoch: {i+1} Loss: {self.history['loss'][-1]} Accuracy: {self.history['accuracy'][-1]}\")\n",
    "                    # calculate loss\n",
    "                    self.backPropogation(loss['loss'],lr)\n",
    "\n",
    "    def forWord(self, layer, input):\n",
    "        return np.dot(layer.weight, input) + layer.bias\n",
    "\n",
    "    def backPropogation(self, loss,lr):\n",
    "        layer_length = len(self.layers)\n",
    "        for index, layer in enumerate(self.layers[::-1]):\n",
    "            if not layer.isInput:\n",
    "                pervious_node = self.layers[layer_length - index - 2]\n",
    "                oldw = np.copy(layer.weight)\n",
    "                layer.weight, layer.bias = self.gradient(\n",
    "                    layer.weight, layer.bias, pervious_node.output, loss,lr)\n",
    "                loss = Error.hiddenError(oldw, loss)\n",
    "\n",
    "    def gradient(self, w, b, x, err, lr=0.01):\n",
    "        w = w - (1/len(x[0]) * (err.dot(x.T))) * lr\n",
    "        b = b - (lr * np.mean(err, axis=1).reshape(b.shape))\n",
    "        return w,b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 2 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 3 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 4 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 5 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 6 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 7 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 8 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 9 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n",
      "Epoch: 10 Loss: 0.35405090024666674 Accuracy: 0.1031904761904762\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = NeuralNetwork(np.copy(x),np.copy(y),Error.crossEntropyLoss)\n",
    "\n",
    "model.append(10, Activation.ReLU)\n",
    "model.append(10,Activation.softmax)\n",
    "model.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.10460289562838226 Accuracy: 0.0670952380952381\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,10) (784,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m, Activation\u001b[39m.\u001b[39mReLU)\n\u001b[1;32m      4\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m,Activation\u001b[39m.\u001b[39msoftmax)\n\u001b[0;32m----> 5\u001b[0m model1\u001b[39m.\u001b[39;49mtrain(\u001b[39m10\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, epoch, lr)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m Accuracy: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# calculate loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackPropogation(loss[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m],lr)\n",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mNeuralNetwork.backPropogation\u001b[0;34m(self, loss, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m pervious_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[layer_length \u001b[39m-\u001b[39m index \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m     41\u001b[0m oldw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(layer\u001b[39m.\u001b[39mweight)\n\u001b[0;32m---> 42\u001b[0m layer\u001b[39m.\u001b[39mweight, layer\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(\n\u001b[1;32m     43\u001b[0m     layer\u001b[39m.\u001b[39;49mweight, layer\u001b[39m.\u001b[39;49mbias, pervious_node\u001b[39m.\u001b[39;49moutput, loss,lr)\n\u001b[1;32m     44\u001b[0m loss \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39mhiddenError(oldw, loss)\n",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m, in \u001b[0;36mNeuralNetwork.gradient\u001b[0;34m(self, w, b, x, err, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(\u001b[39mself\u001b[39m, w, b, x, err, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39;49m (\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39mlen\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]) \u001b[39m*\u001b[39;49m (err\u001b[39m.\u001b[39;49mdot(x\u001b[39m.\u001b[39;49mT))) \u001b[39m*\u001b[39;49m lr\n\u001b[1;32m     48\u001b[0m     b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m (lr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(err, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(b\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m w,b\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,10) (784,10) "
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetwork(np.copy(x),np.copy(y),Error.meanSquareError)\n",
    "\n",
    "model1.append(10, Activation.ReLU)\n",
    "model1.append(10,Activation.softmax)\n",
    "model1.train(10, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(epoch, loss, lr=0.1):\n",
    "    hw = np.random.rand(10, 784) - 0.5\n",
    "    hb = np.random.rand(10, 1) - 0.5\n",
    "    ow = np.random.rand(10, 10) - 0.5\n",
    "    ob = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    def forWord(x,w,b):\n",
    "        return np.dot(w, x) + b\n",
    "\n",
    "    def backWord(err, w,b, x,lr = 0.01):\n",
    "        m = 1/len(x[0])\n",
    "        w = w - (m * (err.dot(x.T))) * lr\n",
    "        b = b - (lr * np.mean(err, axis=1).reshape(b.shape))\n",
    "        return w, b\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        \n",
    "        #forword\n",
    "        #Hidden\n",
    "        hid_op = forWord(x,hw,hb)\n",
    "        hid_act = Activation.ReLU(hid_op)\n",
    "\n",
    "        #op layer\n",
    "        op = forWord(hid_act,ow,ob)\n",
    "        y_pred = Activation.softmax(op)\n",
    "\n",
    "        # backword\n",
    "        # output error\n",
    "        op_err = loss(y, y_pred, True)\n",
    "        ow,ob = backWord(op_err, ow, ob, hid_act, lr)\n",
    "\n",
    "        # #hidden error\n",
    "        hid_err = Error.hiddenError(ow,op_err) * Activation.ReLU(hid_op,True)\n",
    "        hw,hb = backWord(hid_err, hw, hb, x, lr)\n",
    "\n",
    "        err = loss(y, y_pred)\n",
    "        print(f\"Epoch : {i + 1}, Loss : {np.mean(err['loss'])}, Accuracy : {np.mean(err['accuracy'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.48327715779022967, Accuracy : 0.09114285714285714\n",
      "Epoch : 2, Loss : 0.36077419788809156, Accuracy : 0.1044047619047619\n",
      "Epoch : 3, Loss : 0.31645499197587157, Accuracy : 0.11719047619047619\n",
      "Epoch : 4, Loss : 0.29324319185472597, Accuracy : 0.124\n",
      "Epoch : 5, Loss : 0.27839410623951666, Accuracy : 0.12897619047619047\n",
      "Epoch : 6, Loss : 0.26768106270724157, Accuracy : 0.13438095238095238\n",
      "Epoch : 7, Loss : 0.25938369400096106, Accuracy : 0.1397857142857143\n",
      "Epoch : 8, Loss : 0.2526721268507558, Accuracy : 0.14445238095238094\n",
      "Epoch : 9, Loss : 0.24707817313720984, Accuracy : 0.1505\n",
      "Epoch : 10, Loss : 0.2423105299922805, Accuracy : 0.15566666666666668\n",
      "Epoch : 11, Loss : 0.2381784028828643, Accuracy : 0.16097619047619047\n",
      "Epoch : 12, Loss : 0.23453608061688297, Accuracy : 0.1666190476190476\n",
      "Epoch : 13, Loss : 0.23128051514721154, Accuracy : 0.1724047619047619\n",
      "Epoch : 14, Loss : 0.2283263049899599, Accuracy : 0.17716666666666667\n",
      "Epoch : 15, Loss : 0.22561363898383063, Accuracy : 0.18252380952380953\n",
      "Epoch : 16, Loss : 0.2230999144057311, Accuracy : 0.1895238095238095\n",
      "Epoch : 17, Loss : 0.2207368145105469, Accuracy : 0.19592857142857142\n",
      "Epoch : 18, Loss : 0.21849465803165824, Accuracy : 0.2032857142857143\n",
      "Epoch : 19, Loss : 0.21634736198577145, Accuracy : 0.20971428571428571\n",
      "Epoch : 20, Loss : 0.21427026656057666, Accuracy : 0.21726190476190477\n",
      "Epoch : 21, Loss : 0.2122500746231591, Accuracy : 0.22597619047619047\n",
      "Epoch : 22, Loss : 0.21027121798939388, Accuracy : 0.23523809523809525\n",
      "Epoch : 23, Loss : 0.20832105981820856, Accuracy : 0.24585714285714286\n",
      "Epoch : 24, Loss : 0.20639096304112442, Accuracy : 0.255047619047619\n",
      "Epoch : 25, Loss : 0.20447976430255416, Accuracy : 0.2657857142857143\n",
      "Epoch : 26, Loss : 0.20257642359697972, Accuracy : 0.27626190476190476\n",
      "Epoch : 27, Loss : 0.200680602204902, Accuracy : 0.28833333333333333\n",
      "Epoch : 28, Loss : 0.1987940884315225, Accuracy : 0.29928571428571427\n",
      "Epoch : 29, Loss : 0.1969117728820978, Accuracy : 0.31035714285714283\n",
      "Epoch : 30, Loss : 0.19503529472898992, Accuracy : 0.3227857142857143\n",
      "Epoch : 31, Loss : 0.19315962560824163, Accuracy : 0.3341904761904762\n",
      "Epoch : 32, Loss : 0.19128751399768676, Accuracy : 0.3453809523809524\n",
      "Epoch : 33, Loss : 0.18942222150124927, Accuracy : 0.3563095238095238\n",
      "Epoch : 34, Loss : 0.18756715607480676, Accuracy : 0.366452380952381\n",
      "Epoch : 35, Loss : 0.1857218511276838, Accuracy : 0.3760952380952381\n",
      "Epoch : 36, Loss : 0.18388479489518847, Accuracy : 0.3848095238095238\n",
      "Epoch : 37, Loss : 0.18206168146582372, Accuracy : 0.3927857142857143\n",
      "Epoch : 38, Loss : 0.18025419054353772, Accuracy : 0.4000952380952381\n",
      "Epoch : 39, Loss : 0.17846605388330586, Accuracy : 0.4077857142857143\n",
      "Epoch : 40, Loss : 0.1766956062229855, Accuracy : 0.41345238095238096\n",
      "Epoch : 41, Loss : 0.1749464108480907, Accuracy : 0.4190952380952381\n",
      "Epoch : 42, Loss : 0.1732207053959844, Accuracy : 0.42614285714285716\n",
      "Epoch : 43, Loss : 0.17151359206964092, Accuracy : 0.4326904761904762\n",
      "Epoch : 44, Loss : 0.16982658636304346, Accuracy : 0.43811904761904763\n",
      "Epoch : 45, Loss : 0.16816458289105587, Accuracy : 0.4436428571428571\n",
      "Epoch : 46, Loss : 0.16652869858299973, Accuracy : 0.4491904761904762\n",
      "Epoch : 47, Loss : 0.1649187383394279, Accuracy : 0.45395238095238094\n",
      "Epoch : 48, Loss : 0.16333512091704538, Accuracy : 0.4593333333333333\n",
      "Epoch : 49, Loss : 0.16177741954780442, Accuracy : 0.4642380952380952\n",
      "Epoch : 50, Loss : 0.16024296050377088, Accuracy : 0.4698809523809524\n",
      "Epoch : 51, Loss : 0.15873501286979136, Accuracy : 0.4741190476190476\n",
      "Epoch : 52, Loss : 0.15725614227068618, Accuracy : 0.4790952380952381\n",
      "Epoch : 53, Loss : 0.15580403696254908, Accuracy : 0.4838571428571429\n",
      "Epoch : 54, Loss : 0.15437845948543072, Accuracy : 0.48797619047619045\n",
      "Epoch : 55, Loss : 0.15297891331840355, Accuracy : 0.49276190476190473\n",
      "Epoch : 56, Loss : 0.15160194890348758, Accuracy : 0.49776190476190474\n",
      "Epoch : 57, Loss : 0.15024904740246128, Accuracy : 0.5020476190476191\n",
      "Epoch : 58, Loss : 0.14891948468369143, Accuracy : 0.5068571428571429\n",
      "Epoch : 59, Loss : 0.14761180857802883, Accuracy : 0.5112619047619048\n",
      "Epoch : 60, Loss : 0.14632515957807615, Accuracy : 0.5154285714285715\n",
      "Epoch : 61, Loss : 0.14506074276337969, Accuracy : 0.5197857142857143\n",
      "Epoch : 62, Loss : 0.14381827838099195, Accuracy : 0.5240476190476191\n",
      "Epoch : 63, Loss : 0.14259581019057932, Accuracy : 0.5276190476190477\n",
      "Epoch : 64, Loss : 0.14139355084899644, Accuracy : 0.5318571428571428\n",
      "Epoch : 65, Loss : 0.14020998112021024, Accuracy : 0.5353571428571429\n",
      "Epoch : 66, Loss : 0.1390446729113388, Accuracy : 0.5391190476190476\n",
      "Epoch : 67, Loss : 0.1378955426502404, Accuracy : 0.543404761904762\n",
      "Epoch : 68, Loss : 0.13676284072544292, Accuracy : 0.5471904761904762\n",
      "Epoch : 69, Loss : 0.1356459759860113, Accuracy : 0.5515952380952381\n",
      "Epoch : 70, Loss : 0.13454539491339043, Accuracy : 0.555952380952381\n",
      "Epoch : 71, Loss : 0.13346198595922085, Accuracy : 0.5601904761904762\n",
      "Epoch : 72, Loss : 0.1323941353108578, Accuracy : 0.5643809523809524\n",
      "Epoch : 73, Loss : 0.13134235729664295, Accuracy : 0.5683809523809524\n",
      "Epoch : 74, Loss : 0.13030590431821787, Accuracy : 0.5721428571428572\n",
      "Epoch : 75, Loss : 0.12928333753135832, Accuracy : 0.5760952380952381\n",
      "Epoch : 76, Loss : 0.12827341583064242, Accuracy : 0.5801666666666667\n",
      "Epoch : 77, Loss : 0.1272759211189538, Accuracy : 0.5835952380952381\n",
      "Epoch : 78, Loss : 0.12629183928120538, Accuracy : 0.5866428571428571\n",
      "Epoch : 79, Loss : 0.12532194527648519, Accuracy : 0.5900714285714286\n",
      "Epoch : 80, Loss : 0.12436535120609775, Accuracy : 0.5939523809523809\n",
      "Epoch : 81, Loss : 0.12342027464412933, Accuracy : 0.5975714285714285\n",
      "Epoch : 82, Loss : 0.12248780061529575, Accuracy : 0.6004761904761905\n",
      "Epoch : 83, Loss : 0.12156674160026608, Accuracy : 0.604047619047619\n",
      "Epoch : 84, Loss : 0.12065825740877965, Accuracy : 0.607\n",
      "Epoch : 85, Loss : 0.11976217835882239, Accuracy : 0.6105\n",
      "Epoch : 86, Loss : 0.1188767711690846, Accuracy : 0.6140952380952381\n",
      "Epoch : 87, Loss : 0.1180021441007879, Accuracy : 0.6171428571428571\n",
      "Epoch : 88, Loss : 0.11713876874293673, Accuracy : 0.6196904761904762\n",
      "Epoch : 89, Loss : 0.11628591464969878, Accuracy : 0.6226190476190476\n",
      "Epoch : 90, Loss : 0.11544426790355124, Accuracy : 0.6256904761904762\n",
      "Epoch : 91, Loss : 0.1146143440455344, Accuracy : 0.6284047619047619\n",
      "Epoch : 92, Loss : 0.11379581834417955, Accuracy : 0.6318571428571429\n",
      "Epoch : 93, Loss : 0.11298827177358715, Accuracy : 0.6349047619047619\n",
      "Epoch : 94, Loss : 0.11219178390895283, Accuracy : 0.6377380952380952\n",
      "Epoch : 95, Loss : 0.11140634337237446, Accuracy : 0.640452380952381\n",
      "Epoch : 96, Loss : 0.11063262650551889, Accuracy : 0.6434761904761904\n",
      "Epoch : 97, Loss : 0.109871255832635, Accuracy : 0.6457380952380952\n",
      "Epoch : 98, Loss : 0.10912178738400403, Accuracy : 0.6485952380952381\n",
      "Epoch : 99, Loss : 0.1083829905602343, Accuracy : 0.6509285714285714\n",
      "Epoch : 100, Loss : 0.10765480784062555, Accuracy : 0.6535238095238095\n"
     ]
    }
   ],
   "source": [
    "debug(100,Error.crossEntropyLoss,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.11869653794026085, Accuracy : 0.0884047619047619\n",
      "Epoch : 2, Loss : 0.11869644640788884, Accuracy : 0.0884047619047619\n",
      "Epoch : 3, Loss : 0.11869635487570838, Accuracy : 0.0884047619047619\n",
      "Epoch : 4, Loss : 0.11869626334406605, Accuracy : 0.0884047619047619\n",
      "Epoch : 5, Loss : 0.11869617181296181, Accuracy : 0.0884047619047619\n",
      "Epoch : 6, Loss : 0.1186960802823957, Accuracy : 0.0884047619047619\n",
      "Epoch : 7, Loss : 0.11869598875236766, Accuracy : 0.0884047619047619\n",
      "Epoch : 8, Loss : 0.11869589722287772, Accuracy : 0.0884047619047619\n",
      "Epoch : 9, Loss : 0.11869580569392589, Accuracy : 0.0884047619047619\n",
      "Epoch : 10, Loss : 0.11869571416551218, Accuracy : 0.0884047619047619\n",
      "Epoch : 11, Loss : 0.11869562263773854, Accuracy : 0.0884047619047619\n",
      "Epoch : 12, Loss : 0.11869553111050966, Accuracy : 0.0884047619047619\n",
      "Epoch : 13, Loss : 0.1186954395838189, Accuracy : 0.0884047619047619\n",
      "Epoch : 14, Loss : 0.11869534805766617, Accuracy : 0.0884047619047619\n",
      "Epoch : 15, Loss : 0.1186952565320516, Accuracy : 0.0884047619047619\n",
      "Epoch : 16, Loss : 0.11869516500707974, Accuracy : 0.0884047619047619\n",
      "Epoch : 17, Loss : 0.1186950734828378, Accuracy : 0.0884047619047619\n",
      "Epoch : 18, Loss : 0.11869498195913392, Accuracy : 0.0884047619047619\n",
      "Epoch : 19, Loss : 0.11869489043596806, Accuracy : 0.0884047619047619\n",
      "Epoch : 20, Loss : 0.11869479891334034, Accuracy : 0.0884047619047619\n",
      "Epoch : 21, Loss : 0.11869470739125051, Accuracy : 0.0884047619047619\n",
      "Epoch : 22, Loss : 0.11869461586969884, Accuracy : 0.0884047619047619\n",
      "Epoch : 23, Loss : 0.11869452434868517, Accuracy : 0.0884047619047619\n",
      "Epoch : 24, Loss : 0.11869443282820953, Accuracy : 0.0884047619047619\n",
      "Epoch : 25, Loss : 0.11869434130827194, Accuracy : 0.0884047619047619\n",
      "Epoch : 26, Loss : 0.11869424978887236, Accuracy : 0.0884047619047619\n",
      "Epoch : 27, Loss : 0.11869415826896075, Accuracy : 0.0884047619047619\n",
      "Epoch : 28, Loss : 0.11869406674910701, Accuracy : 0.0884047619047619\n",
      "Epoch : 29, Loss : 0.11869397522979137, Accuracy : 0.0884047619047619\n",
      "Epoch : 30, Loss : 0.11869388371101366, Accuracy : 0.0884047619047619\n",
      "Epoch : 31, Loss : 0.11869379219279073, Accuracy : 0.0884047619047619\n",
      "Epoch : 32, Loss : 0.11869370067534242, Accuracy : 0.0884047619047619\n",
      "Epoch : 33, Loss : 0.11869360915843206, Accuracy : 0.0884047619047619\n",
      "Epoch : 34, Loss : 0.11869351764205978, Accuracy : 0.0884047619047619\n",
      "Epoch : 35, Loss : 0.1186934261262254, Accuracy : 0.0884047619047619\n",
      "Epoch : 36, Loss : 0.118693334610929, Accuracy : 0.0884047619047619\n",
      "Epoch : 37, Loss : 0.11869324309617467, Accuracy : 0.0884047619047619\n",
      "Epoch : 38, Loss : 0.11869315158224335, Accuracy : 0.0884047619047619\n",
      "Epoch : 39, Loss : 0.11869306006884994, Accuracy : 0.0884047619047619\n",
      "Epoch : 40, Loss : 0.11869296855599454, Accuracy : 0.0884047619047619\n",
      "Epoch : 41, Loss : 0.1186928770436771, Accuracy : 0.0884047619047619\n",
      "Epoch : 42, Loss : 0.11869278553189765, Accuracy : 0.0884047619047619\n",
      "Epoch : 43, Loss : 0.11869269402065614, Accuracy : 0.0884047619047619\n",
      "Epoch : 44, Loss : 0.11869260250995256, Accuracy : 0.0884047619047619\n",
      "Epoch : 45, Loss : 0.11869251099978693, Accuracy : 0.0884047619047619\n",
      "Epoch : 46, Loss : 0.11869241949015927, Accuracy : 0.0884047619047619\n",
      "Epoch : 47, Loss : 0.11869232798106948, Accuracy : 0.0884047619047619\n",
      "Epoch : 48, Loss : 0.11869223647251771, Accuracy : 0.0884047619047619\n",
      "Epoch : 49, Loss : 0.11869214496450384, Accuracy : 0.0884047619047619\n",
      "Epoch : 50, Loss : 0.11869205345703215, Accuracy : 0.0884047619047619\n",
      "Epoch : 51, Loss : 0.1186919619503829, Accuracy : 0.0884047619047619\n",
      "Epoch : 52, Loss : 0.1186918704442714, Accuracy : 0.0884047619047619\n",
      "Epoch : 53, Loss : 0.11869177893869795, Accuracy : 0.0884047619047619\n",
      "Epoch : 54, Loss : 0.11869168743366236, Accuracy : 0.0884047619047619\n",
      "Epoch : 55, Loss : 0.11869159592916467, Accuracy : 0.0884047619047619\n",
      "Epoch : 56, Loss : 0.11869150442469614, Accuracy : 0.0884047619047619\n",
      "Epoch : 57, Loss : 0.11869141292014594, Accuracy : 0.0884047619047619\n",
      "Epoch : 58, Loss : 0.11869132141613369, Accuracy : 0.0884047619047619\n",
      "Epoch : 59, Loss : 0.11869122991265928, Accuracy : 0.0884047619047619\n",
      "Epoch : 60, Loss : 0.11869113840972285, Accuracy : 0.0884047619047619\n",
      "Epoch : 61, Loss : 0.11869104690732425, Accuracy : 0.0884047619047619\n",
      "Epoch : 62, Loss : 0.11869095540546357, Accuracy : 0.0884047619047619\n",
      "Epoch : 63, Loss : 0.1186908639041348, Accuracy : 0.0884047619047619\n",
      "Epoch : 64, Loss : 0.11869077240314752, Accuracy : 0.0884047619047619\n",
      "Epoch : 65, Loss : 0.11869068090283755, Accuracy : 0.0884047619047619\n",
      "Epoch : 66, Loss : 0.118690589403523, Accuracy : 0.0884047619047619\n",
      "Epoch : 67, Loss : 0.11869049790468235, Accuracy : 0.0884047619047619\n",
      "Epoch : 68, Loss : 0.1186904064063424, Accuracy : 0.0884047619047619\n",
      "Epoch : 69, Loss : 0.11869031490854035, Accuracy : 0.0884047619047619\n",
      "Epoch : 70, Loss : 0.11869022341127612, Accuracy : 0.0884047619047619\n",
      "Epoch : 71, Loss : 0.11869013191454975, Accuracy : 0.0884047619047619\n",
      "Epoch : 72, Loss : 0.11869004041836119, Accuracy : 0.08838095238095238\n",
      "Epoch : 73, Loss : 0.11868994892271048, Accuracy : 0.08838095238095238\n",
      "Epoch : 74, Loss : 0.11868985742759758, Accuracy : 0.08838095238095238\n",
      "Epoch : 75, Loss : 0.11868976593302255, Accuracy : 0.08838095238095238\n",
      "Epoch : 76, Loss : 0.1186896744392116, Accuracy : 0.08838095238095238\n",
      "Epoch : 77, Loss : 0.11868958294719696, Accuracy : 0.08838095238095238\n",
      "Epoch : 78, Loss : 0.11868949145572011, Accuracy : 0.08838095238095238\n",
      "Epoch : 79, Loss : 0.11868939996478103, Accuracy : 0.08838095238095238\n",
      "Epoch : 80, Loss : 0.11868930847437982, Accuracy : 0.08838095238095238\n",
      "Epoch : 81, Loss : 0.11868921698451633, Accuracy : 0.08838095238095238\n",
      "Epoch : 82, Loss : 0.11868912549519069, Accuracy : 0.08838095238095238\n",
      "Epoch : 83, Loss : 0.11868903400640285, Accuracy : 0.08838095238095238\n",
      "Epoch : 84, Loss : 0.11868894251815282, Accuracy : 0.08838095238095238\n",
      "Epoch : 85, Loss : 0.11868885103044054, Accuracy : 0.08838095238095238\n",
      "Epoch : 86, Loss : 0.11868875954326599, Accuracy : 0.08838095238095238\n",
      "Epoch : 87, Loss : 0.11868866805662932, Accuracy : 0.08838095238095238\n",
      "Epoch : 88, Loss : 0.1186885765705304, Accuracy : 0.08838095238095238\n",
      "Epoch : 89, Loss : 0.11868848508496925, Accuracy : 0.08838095238095238\n",
      "Epoch : 90, Loss : 0.11868839360062027, Accuracy : 0.08838095238095238\n",
      "Epoch : 91, Loss : 0.11868830211746428, Accuracy : 0.08838095238095238\n",
      "Epoch : 92, Loss : 0.11868821063484598, Accuracy : 0.08838095238095238\n",
      "Epoch : 93, Loss : 0.11868811915276548, Accuracy : 0.08838095238095238\n",
      "Epoch : 94, Loss : 0.11868802767122277, Accuracy : 0.08838095238095238\n",
      "Epoch : 95, Loss : 0.11868793619021768, Accuracy : 0.08838095238095238\n",
      "Epoch : 96, Loss : 0.11868784470975043, Accuracy : 0.08838095238095238\n",
      "Epoch : 97, Loss : 0.11868775322982095, Accuracy : 0.08838095238095238\n",
      "Epoch : 98, Loss : 0.11868766175042912, Accuracy : 0.08838095238095238\n",
      "Epoch : 99, Loss : 0.11868757027157505, Accuracy : 0.08838095238095238\n",
      "Epoch : 100, Loss : 0.11868747879325874, Accuracy : 0.08838095238095238\n"
     ]
    }
   ],
   "source": [
    "debug(100,Error.meanSquareError,0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
