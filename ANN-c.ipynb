{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    '''\n",
    "    MNIST data set \n",
    "    x has 784 feature\n",
    "    y is op value from 0 to 9 \n",
    "    '''\n",
    "    data = np.array(pd.read_csv('data/MNIST/MNIST_train.csv'))\n",
    "    x = (data[:,1:]/255.).T\n",
    "    y = oneHot(data[:,0])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def ReLU(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return Z > 0\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            op = Activation.sigmoid(Z)\n",
    "            return op * (1- op)\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z,isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 1\n",
    "        Z = Z - np.max(Z, axis=0)\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    @staticmethod\n",
    "    def multiClass(target, prediction):\n",
    "        return np.argmax(target, axis=0) == np.argmax(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error:\n",
    "    @staticmethod\n",
    "    def meanSquareError(target, predicted, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 2 * (predicted - target) / np.size(target)\n",
    "        loss = np.power(target - predicted, 2)\n",
    "        accuracy = Accuracy.multiClass(target, predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(target, predicted,  isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return predicted - target\n",
    "        loss = -target * np.log(predicted + 10 ** -100)\n",
    "        accuracy = Accuracy.multiClass(target,predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def hiddenError(target, predicted):\n",
    "        return target.T.dot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    op = []\n",
    "    bias = []\n",
    "    weight = []\n",
    "    output = []\n",
    "    isInput=False\n",
    "    noOfNodes = 0\n",
    "    activation = None\n",
    "    def __init__(self, inputSize=0, outputSize=0, activation=None,isInput=False,input=[]):\n",
    "        '''\n",
    "        inputSize -> no.of.input feature \n",
    "        outputSize -> no.of.output\n",
    "        '''\n",
    "        if isInput:\n",
    "            self.output = input\n",
    "            self.isInput = True\n",
    "            self.noOfNodes = input.shape[0]\n",
    "        else :\n",
    "            self.noOfNodes = inputSize\n",
    "            self.activation = activation\n",
    "            self.weight = np.random.rand(inputSize, outputSize) - 0.5\n",
    "            self.bias = np.random.rand(inputSize,1) - 0.5\n",
    "\n",
    "    def generateWeight(self,*r):\n",
    "        '''\n",
    "        Receive input as set that define the set shape\n",
    "        '''\n",
    "        return np.random.randn(*r) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    input = []\n",
    "    target = []\n",
    "    layers = []\n",
    "    history = {'loss': [], 'accuracy':[]}\n",
    "    loss = None\n",
    "\n",
    "    @staticmethod\n",
    "    def forWord(w, b, x):\n",
    "        return np.dot(w, x) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(w, b, x, err, lr=0.01):\n",
    "        w = w - (1/len(x[0]) * (err.dot(x.T))) * lr\n",
    "        b = b - (lr * np.mean(err, axis=1).reshape(b.shape))\n",
    "        return w,b  \n",
    "\n",
    "    def __init__(self, input, target, loss):\n",
    "        self.loss = loss\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.layers.append(\n",
    "            DenseLayer(None,None,None,isInput=True, input=self.input)\n",
    "        )\n",
    "\n",
    "    def append(self, node, activationFunction):\n",
    "        preNode = self.layers[len(self.layers) -1]\n",
    "        self.layers.append(\n",
    "            DenseLayer(inputSize=node,outputSize=preNode.noOfNodes, activation=activationFunction)\n",
    "        )\n",
    "\n",
    "    def train(self, epoch=100, lr=0.01):\n",
    "        for i in range(epoch):\n",
    "\n",
    "            for j in range(len(self.layers)):\n",
    "\n",
    "                if not self.layers[j].isInput:\n",
    "                    self.layers[j].op = NeuralNetwork.forWord(self.layers[j].weight, self.layers[j].bias, self.layers[j-1].output)\n",
    "                    self.layers[j].output = self.layers[j].activation(self.layers[j].op)\n",
    "\n",
    "                if j == len(self.layers)-1 :\n",
    "                    loss = self.loss(self.target,self.layers[j].output)\n",
    "                    self.history['loss'].append(np.mean(loss['loss']))\n",
    "                    self.history['accuracy'].append(np.mean(loss['accuracy']))\n",
    "                    print(f\"Epoch: {i+1} Loss: {self.history['loss'][-1]} Accuracy: {self.history['accuracy'][-1]}\")\n",
    "                    # calculate loss\n",
    "                    self.backPropogation(loss['loss'],lr)\n",
    "\n",
    "\n",
    "    def backPropogation(self, loss,lr):\n",
    "        layer_length = len(self.layers)\n",
    "        for index, layer in enumerate(self.layers[::-1]):\n",
    "            if not layer.isInput:\n",
    "                pervious_node = self.layers[layer_length - index - 2]\n",
    "\n",
    "                layer.weight, layer.bias = NeuralNetwork.gradient(\n",
    "                    w=layer.weight, b=layer.bias, x=pervious_node.output, err=loss,lr=lr)\n",
    "                    \n",
    "                oldw = np.copy(layer.weight)\n",
    "\n",
    "                loss = Error.hiddenError(oldw, loss)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer :  1 9.695222881785638 0.0\n",
      "Layer :  2 0.8758029390824879 4.5009360749428186e-06\n",
      "Epoch: 1 Loss: 0.2840997948748314 Accuracy: 0.08061904761904762\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.691253650337572 0.0\n",
      "Layer :  2 0.8783575991244833 4.327687858149742e-06\n",
      "Epoch: 2 Loss: 0.285423818488201 Accuracy: 0.08028571428571428\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.696799199368703 0.0\n",
      "Layer :  2 0.8872659220361144 3.9217769070783815e-06\n",
      "Epoch: 3 Loss: 0.286973191037366 Accuracy: 0.07980952380952382\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.710681650097998 0.0\n",
      "Layer :  2 0.8958455142728211 3.492410184188924e-06\n",
      "Epoch: 4 Loss: 0.2887759890707759 Accuracy: 0.08021428571428571\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.733333253383385 0.0\n",
      "Layer :  2 0.9039815809889961 3.0474377473614095e-06\n",
      "Epoch: 5 Loss: 0.2908687868943239 Accuracy: 0.08033333333333334\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.765292301616135 0.0\n",
      "Layer :  2 0.909089675084675 2.5969294149774826e-06\n",
      "Epoch: 6 Loss: 0.29327078808788115 Accuracy: 0.08061904761904762\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.807219961695028 0.0\n",
      "Layer :  2 0.9143202073501778 2.1530680753140863e-06\n",
      "Epoch: 7 Loss: 0.29602704974607513 Accuracy: 0.08085714285714286\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.859924719892437 0.0\n",
      "Layer :  2 0.9196651078183846 1.7290838254922023e-06\n",
      "Epoch: 8 Loss: 0.29917991698551044 Accuracy: 0.08064285714285714\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 9.924391121043312 0.0\n",
      "Layer :  2 0.925111979735681 1.3380484688137289e-06\n",
      "Epoch: 9 Loss: 0.30279877207770584 Accuracy: 0.08154761904761905\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Layer :  1 10.001822992480927 0.0\n",
      "Layer :  2 0.930644995277718 9.915404661253634e-07\n",
      "Epoch: 10 Loss: 0.30697305234954886 Accuracy: 0.08221428571428571\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(np.copy(x),np.copy(y),Error.crossEntropyLoss)\n",
    "\n",
    "model.append(10, Activation.ReLU)\n",
    "model.append(10,Activation.softmax)\n",
    "model.train(10)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.09957427570808244 Accuracy: 0.10680952380952381\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n",
      "Current Layer Weight :  (10, 784)  Pre layer Weight :  0\n",
      "Current Layer Weight :  (10, 10)  Pre layer Weight :  (10, 784)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,10) (784,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m, Activation\u001b[39m.\u001b[39mReLU)\n\u001b[1;32m      4\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m,Activation\u001b[39m.\u001b[39msoftmax)\n\u001b[0;32m----> 5\u001b[0m model1\u001b[39m.\u001b[39;49mtrain(\u001b[39m10\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39mdel\u001b[39;00m model1\n",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, epoch, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m Accuracy: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39m# calculate loss\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackPropogation(loss[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m],lr)\n",
      "Cell \u001b[0;32mIn[8], line 58\u001b[0m, in \u001b[0;36mNeuralNetwork.backPropogation\u001b[0;34m(self, loss, lr)\u001b[0m\n\u001b[1;32m     55\u001b[0m pervious_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[layer_length \u001b[39m-\u001b[39m index \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Layer Weight : \u001b[39m\u001b[39m\"\u001b[39m,layer\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape, \u001b[39m\"\u001b[39m\u001b[39m Pre layer Weight : \u001b[39m\u001b[39m\"\u001b[39m,pervious_node\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pervious_node\u001b[39m.\u001b[39mweight) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m layer\u001b[39m.\u001b[39mweight, layer\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m NeuralNetwork\u001b[39m.\u001b[39;49mgradient(\n\u001b[1;32m     59\u001b[0m     w\u001b[39m=\u001b[39;49mlayer\u001b[39m.\u001b[39;49mweight, b\u001b[39m=\u001b[39;49mlayer\u001b[39m.\u001b[39;49mbias, x\u001b[39m=\u001b[39;49mpervious_node\u001b[39m.\u001b[39;49moutput, err\u001b[39m=\u001b[39;49mloss,lr\u001b[39m=\u001b[39;49mlr)\n\u001b[1;32m     61\u001b[0m oldw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(layer\u001b[39m.\u001b[39mweight)\n\u001b[1;32m     63\u001b[0m loss \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39mhiddenError(oldw, loss)\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mNeuralNetwork.gradient\u001b[0;34m(w, b, x, err, lr)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(w, b, x, err, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39;49m (\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39mlen\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]) \u001b[39m*\u001b[39;49m (err\u001b[39m.\u001b[39;49mdot(x\u001b[39m.\u001b[39;49mT))) \u001b[39m*\u001b[39;49m lr\n\u001b[1;32m     15\u001b[0m     b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m (lr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(err, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(b\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m w,b\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,10) (784,10) "
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetwork(np.copy(x),np.copy(y),Error.meanSquareError)\n",
    "\n",
    "model1.append(10, Activation.ReLU)\n",
    "model1.append(10,Activation.softmax)\n",
    "model1.train(10, 0.1)\n",
    "del model1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(epoch, loss, lr=0.1):\n",
    "    hw = np.random.rand(10, 784) - 0.5\n",
    "    hb = np.random.rand(10, 1) - 0.5\n",
    "    ow = np.random.rand(10, 10) - 0.5\n",
    "    ob = np.random.rand(10, 1) - 0.5\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        #forword\n",
    "        #Hidden\n",
    "        hid_op = NeuralNetwork.forWord(hw,hb,x)\n",
    "        hid_act = Activation.ReLU(hid_op)\n",
    "\n",
    "        #op layer\n",
    "        op = NeuralNetwork.forWord(ow,ob,hid_act)\n",
    "        y_pred = Activation.softmax(op)\n",
    "\n",
    "        # backword\n",
    "        # output error\n",
    "        op_err = loss(y, y_pred, True)\n",
    "        ow,ob = NeuralNetwork.gradient(w=ow, b=ob, x=hid_act,err=op_err,lr=lr)\n",
    "\n",
    "        # #hidden error\n",
    "        hid_err = Error.hiddenError(ow,op_err) * Activation.ReLU(hid_op,True)\n",
    "        hw,hb = NeuralNetwork.gradient(w=hw, b=hb, x=x,err=hid_err, lr=lr)\n",
    "\n",
    "        err = loss(y, y_pred)\n",
    "        print(f\"Epoch : {i + 1}, Loss : {np.mean(err['loss'])}, Accuracy : {np.mean(err['accuracy'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.33758894382169613, Accuracy : 0.0838095238095238\n",
      "Epoch : 2, Loss : 0.28991160049882914, Accuracy : 0.09242857142857143\n",
      "Epoch : 3, Loss : 0.2670700248863418, Accuracy : 0.10902380952380952\n",
      "Epoch : 4, Loss : 0.25464183013068625, Accuracy : 0.12511904761904763\n",
      "Epoch : 5, Loss : 0.24693485834664106, Accuracy : 0.13488095238095238\n",
      "Epoch : 6, Loss : 0.2415341027560799, Accuracy : 0.1447142857142857\n",
      "Epoch : 7, Loss : 0.23738793587872198, Accuracy : 0.15345238095238095\n",
      "Epoch : 8, Loss : 0.23400895210867884, Accuracy : 0.16080952380952382\n",
      "Epoch : 9, Loss : 0.2311411959810831, Accuracy : 0.17007142857142857\n",
      "Epoch : 10, Loss : 0.2286387705179894, Accuracy : 0.17714285714285713\n",
      "Epoch : 11, Loss : 0.2264087301638529, Accuracy : 0.18464285714285714\n",
      "Epoch : 12, Loss : 0.22438705811000892, Accuracy : 0.19142857142857142\n",
      "Epoch : 13, Loss : 0.2225278360239665, Accuracy : 0.1981904761904762\n",
      "Epoch : 14, Loss : 0.2207960813909436, Accuracy : 0.20607142857142857\n",
      "Epoch : 15, Loss : 0.2191663271363943, Accuracy : 0.21354761904761904\n",
      "Epoch : 16, Loss : 0.2176191592791035, Accuracy : 0.22085714285714286\n",
      "Epoch : 17, Loss : 0.21613722128322305, Accuracy : 0.22866666666666666\n",
      "Epoch : 18, Loss : 0.21470656513661546, Accuracy : 0.2361904761904762\n",
      "Epoch : 19, Loss : 0.21331759033405095, Accuracy : 0.2425\n",
      "Epoch : 20, Loss : 0.2119614563260216, Accuracy : 0.25016666666666665\n",
      "Epoch : 21, Loss : 0.21063098585807014, Accuracy : 0.2558095238095238\n",
      "Epoch : 22, Loss : 0.2093202837897873, Accuracy : 0.26226190476190475\n",
      "Epoch : 23, Loss : 0.20802511808969174, Accuracy : 0.2689285714285714\n",
      "Epoch : 24, Loss : 0.20673958496614186, Accuracy : 0.2748333333333333\n",
      "Epoch : 25, Loss : 0.20545932889829283, Accuracy : 0.28104761904761905\n",
      "Epoch : 26, Loss : 0.20418132992261304, Accuracy : 0.287\n",
      "Epoch : 27, Loss : 0.20290120799731115, Accuracy : 0.29323809523809524\n",
      "Epoch : 28, Loss : 0.20161821720240602, Accuracy : 0.29878571428571427\n",
      "Epoch : 29, Loss : 0.20033105432029655, Accuracy : 0.305\n",
      "Epoch : 30, Loss : 0.1990329331925868, Accuracy : 0.31064285714285716\n",
      "Epoch : 31, Loss : 0.19772386034119627, Accuracy : 0.3163809523809524\n",
      "Epoch : 32, Loss : 0.1964042662735666, Accuracy : 0.3214047619047619\n",
      "Epoch : 33, Loss : 0.19507136659559624, Accuracy : 0.3273333333333333\n",
      "Epoch : 34, Loss : 0.1937249819264122, Accuracy : 0.3334761904761905\n",
      "Epoch : 35, Loss : 0.19235974501752284, Accuracy : 0.3393095238095238\n",
      "Epoch : 36, Loss : 0.1909759815015822, Accuracy : 0.34454761904761905\n",
      "Epoch : 37, Loss : 0.18957467412846857, Accuracy : 0.3497619047619048\n",
      "Epoch : 38, Loss : 0.1881610461578251, Accuracy : 0.3552142857142857\n",
      "Epoch : 39, Loss : 0.18673691747489715, Accuracy : 0.36078571428571427\n",
      "Epoch : 40, Loss : 0.18529905774366973, Accuracy : 0.36604761904761907\n",
      "Epoch : 41, Loss : 0.183850798844512, Accuracy : 0.37052380952380953\n",
      "Epoch : 42, Loss : 0.18238531602518776, Accuracy : 0.3762142857142857\n",
      "Epoch : 43, Loss : 0.18089999815732036, Accuracy : 0.38176190476190475\n",
      "Epoch : 44, Loss : 0.17939674384999305, Accuracy : 0.38766666666666666\n",
      "Epoch : 45, Loss : 0.17787439536700606, Accuracy : 0.39280952380952383\n",
      "Epoch : 46, Loss : 0.1763392364336788, Accuracy : 0.3979047619047619\n",
      "Epoch : 47, Loss : 0.1747882987982146, Accuracy : 0.40376190476190477\n",
      "Epoch : 48, Loss : 0.17321150124601084, Accuracy : 0.41054761904761905\n",
      "Epoch : 49, Loss : 0.17161762608222736, Accuracy : 0.4165\n",
      "Epoch : 50, Loss : 0.1700020279426977, Accuracy : 0.42228571428571426\n",
      "Epoch : 51, Loss : 0.16835734463965576, Accuracy : 0.42757142857142855\n",
      "Epoch : 52, Loss : 0.16669140986555925, Accuracy : 0.43354761904761907\n",
      "Epoch : 53, Loss : 0.1650083971465428, Accuracy : 0.44045238095238093\n",
      "Epoch : 54, Loss : 0.1633069896081381, Accuracy : 0.44716666666666666\n",
      "Epoch : 55, Loss : 0.1615967483913627, Accuracy : 0.45345238095238094\n",
      "Epoch : 56, Loss : 0.15988797203631167, Accuracy : 0.4599761904761905\n",
      "Epoch : 57, Loss : 0.15819035540332535, Accuracy : 0.4658809523809524\n",
      "Epoch : 58, Loss : 0.156503931741795, Accuracy : 0.4715714285714286\n",
      "Epoch : 59, Loss : 0.15484117812111955, Accuracy : 0.47785714285714287\n",
      "Epoch : 60, Loss : 0.15320355564916058, Accuracy : 0.4839047619047619\n",
      "Epoch : 61, Loss : 0.15159419878481348, Accuracy : 0.4893571428571429\n",
      "Epoch : 62, Loss : 0.15001565735283576, Accuracy : 0.4951904761904762\n",
      "Epoch : 63, Loss : 0.1484640643882442, Accuracy : 0.501\n",
      "Epoch : 64, Loss : 0.1469420006458636, Accuracy : 0.5055238095238095\n",
      "Epoch : 65, Loss : 0.14545052922324483, Accuracy : 0.5118095238095238\n",
      "Epoch : 66, Loss : 0.14398885757524305, Accuracy : 0.5163333333333333\n",
      "Epoch : 67, Loss : 0.14255590939000123, Accuracy : 0.5213095238095238\n",
      "Epoch : 68, Loss : 0.141155220955653, Accuracy : 0.5265714285714286\n",
      "Epoch : 69, Loss : 0.1397877634427894, Accuracy : 0.5314285714285715\n",
      "Epoch : 70, Loss : 0.1384545729208645, Accuracy : 0.5361428571428571\n",
      "Epoch : 71, Loss : 0.13715307910992414, Accuracy : 0.5408571428571428\n",
      "Epoch : 72, Loss : 0.13588050906562157, Accuracy : 0.545047619047619\n",
      "Epoch : 73, Loss : 0.13463660369919808, Accuracy : 0.5489285714285714\n",
      "Epoch : 74, Loss : 0.13342181724150268, Accuracy : 0.5526666666666666\n",
      "Epoch : 75, Loss : 0.1322311826094079, Accuracy : 0.5569285714285714\n",
      "Epoch : 76, Loss : 0.13106455227644523, Accuracy : 0.5608095238095238\n",
      "Epoch : 77, Loss : 0.1299236287006172, Accuracy : 0.5652857142857143\n",
      "Epoch : 78, Loss : 0.12880814236997695, Accuracy : 0.5693333333333334\n",
      "Epoch : 79, Loss : 0.1277161927393374, Accuracy : 0.5726666666666667\n",
      "Epoch : 80, Loss : 0.12664840857856555, Accuracy : 0.5758809523809524\n",
      "Epoch : 81, Loss : 0.1256032340943062, Accuracy : 0.5793333333333334\n",
      "Epoch : 82, Loss : 0.12458107790391329, Accuracy : 0.582952380952381\n",
      "Epoch : 83, Loss : 0.1235834747697218, Accuracy : 0.5859047619047619\n",
      "Epoch : 84, Loss : 0.12260898635133587, Accuracy : 0.5892619047619048\n",
      "Epoch : 85, Loss : 0.12165528658893861, Accuracy : 0.5922619047619048\n",
      "Epoch : 86, Loss : 0.12071972375303362, Accuracy : 0.595\n",
      "Epoch : 87, Loss : 0.11980217271543016, Accuracy : 0.5980714285714286\n",
      "Epoch : 88, Loss : 0.1189045301482958, Accuracy : 0.6013095238095238\n",
      "Epoch : 89, Loss : 0.1180274876958006, Accuracy : 0.604452380952381\n",
      "Epoch : 90, Loss : 0.11716888752507394, Accuracy : 0.607547619047619\n",
      "Epoch : 91, Loss : 0.11632966734698798, Accuracy : 0.6102619047619048\n",
      "Epoch : 92, Loss : 0.11550832805140675, Accuracy : 0.6130714285714286\n",
      "Epoch : 93, Loss : 0.11470398734623717, Accuracy : 0.6158809523809524\n",
      "Epoch : 94, Loss : 0.11391575012621304, Accuracy : 0.6184761904761905\n",
      "Epoch : 95, Loss : 0.1131432369213156, Accuracy : 0.6215952380952381\n",
      "Epoch : 96, Loss : 0.11238667281719965, Accuracy : 0.6241904761904762\n",
      "Epoch : 97, Loss : 0.11164359686681523, Accuracy : 0.6266904761904762\n",
      "Epoch : 98, Loss : 0.11091479422854279, Accuracy : 0.6292142857142857\n",
      "Epoch : 99, Loss : 0.11019875607771744, Accuracy : 0.6317857142857143\n",
      "Epoch : 100, Loss : 0.10949640162575826, Accuracy : 0.6341428571428571\n"
     ]
    }
   ],
   "source": [
    "debug(100,Error.crossEntropyLoss,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.11869653794026085, Accuracy : 0.0884047619047619\n",
      "Epoch : 2, Loss : 0.11869644640788884, Accuracy : 0.0884047619047619\n",
      "Epoch : 3, Loss : 0.11869635487570838, Accuracy : 0.0884047619047619\n",
      "Epoch : 4, Loss : 0.11869626334406605, Accuracy : 0.0884047619047619\n",
      "Epoch : 5, Loss : 0.11869617181296181, Accuracy : 0.0884047619047619\n",
      "Epoch : 6, Loss : 0.1186960802823957, Accuracy : 0.0884047619047619\n",
      "Epoch : 7, Loss : 0.11869598875236766, Accuracy : 0.0884047619047619\n",
      "Epoch : 8, Loss : 0.11869589722287772, Accuracy : 0.0884047619047619\n",
      "Epoch : 9, Loss : 0.11869580569392589, Accuracy : 0.0884047619047619\n",
      "Epoch : 10, Loss : 0.11869571416551218, Accuracy : 0.0884047619047619\n",
      "Epoch : 11, Loss : 0.11869562263773854, Accuracy : 0.0884047619047619\n",
      "Epoch : 12, Loss : 0.11869553111050966, Accuracy : 0.0884047619047619\n",
      "Epoch : 13, Loss : 0.1186954395838189, Accuracy : 0.0884047619047619\n",
      "Epoch : 14, Loss : 0.11869534805766617, Accuracy : 0.0884047619047619\n",
      "Epoch : 15, Loss : 0.1186952565320516, Accuracy : 0.0884047619047619\n",
      "Epoch : 16, Loss : 0.11869516500707974, Accuracy : 0.0884047619047619\n",
      "Epoch : 17, Loss : 0.1186950734828378, Accuracy : 0.0884047619047619\n",
      "Epoch : 18, Loss : 0.11869498195913392, Accuracy : 0.0884047619047619\n",
      "Epoch : 19, Loss : 0.11869489043596806, Accuracy : 0.0884047619047619\n",
      "Epoch : 20, Loss : 0.11869479891334034, Accuracy : 0.0884047619047619\n",
      "Epoch : 21, Loss : 0.11869470739125051, Accuracy : 0.0884047619047619\n",
      "Epoch : 22, Loss : 0.11869461586969884, Accuracy : 0.0884047619047619\n",
      "Epoch : 23, Loss : 0.11869452434868517, Accuracy : 0.0884047619047619\n",
      "Epoch : 24, Loss : 0.11869443282820953, Accuracy : 0.0884047619047619\n",
      "Epoch : 25, Loss : 0.11869434130827194, Accuracy : 0.0884047619047619\n",
      "Epoch : 26, Loss : 0.11869424978887236, Accuracy : 0.0884047619047619\n",
      "Epoch : 27, Loss : 0.11869415826896075, Accuracy : 0.0884047619047619\n",
      "Epoch : 28, Loss : 0.11869406674910701, Accuracy : 0.0884047619047619\n",
      "Epoch : 29, Loss : 0.11869397522979137, Accuracy : 0.0884047619047619\n",
      "Epoch : 30, Loss : 0.11869388371101366, Accuracy : 0.0884047619047619\n",
      "Epoch : 31, Loss : 0.11869379219279073, Accuracy : 0.0884047619047619\n",
      "Epoch : 32, Loss : 0.11869370067534242, Accuracy : 0.0884047619047619\n",
      "Epoch : 33, Loss : 0.11869360915843206, Accuracy : 0.0884047619047619\n",
      "Epoch : 34, Loss : 0.11869351764205978, Accuracy : 0.0884047619047619\n",
      "Epoch : 35, Loss : 0.1186934261262254, Accuracy : 0.0884047619047619\n",
      "Epoch : 36, Loss : 0.118693334610929, Accuracy : 0.0884047619047619\n",
      "Epoch : 37, Loss : 0.11869324309617467, Accuracy : 0.0884047619047619\n",
      "Epoch : 38, Loss : 0.11869315158224335, Accuracy : 0.0884047619047619\n",
      "Epoch : 39, Loss : 0.11869306006884994, Accuracy : 0.0884047619047619\n",
      "Epoch : 40, Loss : 0.11869296855599454, Accuracy : 0.0884047619047619\n",
      "Epoch : 41, Loss : 0.1186928770436771, Accuracy : 0.0884047619047619\n",
      "Epoch : 42, Loss : 0.11869278553189765, Accuracy : 0.0884047619047619\n",
      "Epoch : 43, Loss : 0.11869269402065614, Accuracy : 0.0884047619047619\n",
      "Epoch : 44, Loss : 0.11869260250995256, Accuracy : 0.0884047619047619\n",
      "Epoch : 45, Loss : 0.11869251099978693, Accuracy : 0.0884047619047619\n",
      "Epoch : 46, Loss : 0.11869241949015927, Accuracy : 0.0884047619047619\n",
      "Epoch : 47, Loss : 0.11869232798106948, Accuracy : 0.0884047619047619\n",
      "Epoch : 48, Loss : 0.11869223647251771, Accuracy : 0.0884047619047619\n",
      "Epoch : 49, Loss : 0.11869214496450384, Accuracy : 0.0884047619047619\n",
      "Epoch : 50, Loss : 0.11869205345703215, Accuracy : 0.0884047619047619\n",
      "Epoch : 51, Loss : 0.1186919619503829, Accuracy : 0.0884047619047619\n",
      "Epoch : 52, Loss : 0.1186918704442714, Accuracy : 0.0884047619047619\n",
      "Epoch : 53, Loss : 0.11869177893869795, Accuracy : 0.0884047619047619\n",
      "Epoch : 54, Loss : 0.11869168743366236, Accuracy : 0.0884047619047619\n",
      "Epoch : 55, Loss : 0.11869159592916467, Accuracy : 0.0884047619047619\n",
      "Epoch : 56, Loss : 0.11869150442469614, Accuracy : 0.0884047619047619\n",
      "Epoch : 57, Loss : 0.11869141292014594, Accuracy : 0.0884047619047619\n",
      "Epoch : 58, Loss : 0.11869132141613369, Accuracy : 0.0884047619047619\n",
      "Epoch : 59, Loss : 0.11869122991265928, Accuracy : 0.0884047619047619\n",
      "Epoch : 60, Loss : 0.11869113840972285, Accuracy : 0.0884047619047619\n",
      "Epoch : 61, Loss : 0.11869104690732425, Accuracy : 0.0884047619047619\n",
      "Epoch : 62, Loss : 0.11869095540546357, Accuracy : 0.0884047619047619\n",
      "Epoch : 63, Loss : 0.1186908639041348, Accuracy : 0.0884047619047619\n",
      "Epoch : 64, Loss : 0.11869077240314752, Accuracy : 0.0884047619047619\n",
      "Epoch : 65, Loss : 0.11869068090283755, Accuracy : 0.0884047619047619\n",
      "Epoch : 66, Loss : 0.118690589403523, Accuracy : 0.0884047619047619\n",
      "Epoch : 67, Loss : 0.11869049790468235, Accuracy : 0.0884047619047619\n",
      "Epoch : 68, Loss : 0.1186904064063424, Accuracy : 0.0884047619047619\n",
      "Epoch : 69, Loss : 0.11869031490854035, Accuracy : 0.0884047619047619\n",
      "Epoch : 70, Loss : 0.11869022341127612, Accuracy : 0.0884047619047619\n",
      "Epoch : 71, Loss : 0.11869013191454975, Accuracy : 0.0884047619047619\n",
      "Epoch : 72, Loss : 0.11869004041836119, Accuracy : 0.08838095238095238\n",
      "Epoch : 73, Loss : 0.11868994892271048, Accuracy : 0.08838095238095238\n",
      "Epoch : 74, Loss : 0.11868985742759758, Accuracy : 0.08838095238095238\n",
      "Epoch : 75, Loss : 0.11868976593302255, Accuracy : 0.08838095238095238\n",
      "Epoch : 76, Loss : 0.1186896744392116, Accuracy : 0.08838095238095238\n",
      "Epoch : 77, Loss : 0.11868958294719696, Accuracy : 0.08838095238095238\n",
      "Epoch : 78, Loss : 0.11868949145572011, Accuracy : 0.08838095238095238\n",
      "Epoch : 79, Loss : 0.11868939996478103, Accuracy : 0.08838095238095238\n",
      "Epoch : 80, Loss : 0.11868930847437982, Accuracy : 0.08838095238095238\n",
      "Epoch : 81, Loss : 0.11868921698451633, Accuracy : 0.08838095238095238\n",
      "Epoch : 82, Loss : 0.11868912549519069, Accuracy : 0.08838095238095238\n",
      "Epoch : 83, Loss : 0.11868903400640285, Accuracy : 0.08838095238095238\n",
      "Epoch : 84, Loss : 0.11868894251815282, Accuracy : 0.08838095238095238\n",
      "Epoch : 85, Loss : 0.11868885103044054, Accuracy : 0.08838095238095238\n",
      "Epoch : 86, Loss : 0.11868875954326599, Accuracy : 0.08838095238095238\n",
      "Epoch : 87, Loss : 0.11868866805662932, Accuracy : 0.08838095238095238\n",
      "Epoch : 88, Loss : 0.1186885765705304, Accuracy : 0.08838095238095238\n",
      "Epoch : 89, Loss : 0.11868848508496925, Accuracy : 0.08838095238095238\n",
      "Epoch : 90, Loss : 0.11868839360062027, Accuracy : 0.08838095238095238\n",
      "Epoch : 91, Loss : 0.11868830211746428, Accuracy : 0.08838095238095238\n",
      "Epoch : 92, Loss : 0.11868821063484598, Accuracy : 0.08838095238095238\n",
      "Epoch : 93, Loss : 0.11868811915276548, Accuracy : 0.08838095238095238\n",
      "Epoch : 94, Loss : 0.11868802767122277, Accuracy : 0.08838095238095238\n",
      "Epoch : 95, Loss : 0.11868793619021768, Accuracy : 0.08838095238095238\n",
      "Epoch : 96, Loss : 0.11868784470975043, Accuracy : 0.08838095238095238\n",
      "Epoch : 97, Loss : 0.11868775322982095, Accuracy : 0.08838095238095238\n",
      "Epoch : 98, Loss : 0.11868766175042912, Accuracy : 0.08838095238095238\n",
      "Epoch : 99, Loss : 0.11868757027157505, Accuracy : 0.08838095238095238\n",
      "Epoch : 100, Loss : 0.11868747879325874, Accuracy : 0.08838095238095238\n"
     ]
    }
   ],
   "source": [
    "debug(100,Error.meanSquareError,0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
