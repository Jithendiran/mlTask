{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    '''\n",
    "    MNIST data set \n",
    "    x has 784 feature\n",
    "    y is op value from 0 to 9 \n",
    "    '''\n",
    "    data = np.array(pd.read_csv('data/MNIST/MNIST_train.csv'))\n",
    "    x = (data[:,1:]/255).T\n",
    "    y = oneHot(data[:,0])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def ReLU(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return Z > 0\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            op = Activation.sigmoid(Z)\n",
    "            return op * (1- op)\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z,isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 1\n",
    "        Z = Z - np.max(Z, axis=0)\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    @staticmethod\n",
    "    def multiClass(target, prediction):\n",
    "        return np.argmax(target, axis=0) == np.argmax(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error:\n",
    "    @staticmethod\n",
    "    def meanSquareError(target, predicted, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 2 * (predicted - target) / np.size(target)\n",
    "        loss = np.power(target - predicted, 2)\n",
    "        accuracy = Accuracy.multiClass(target, predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(target, predicted,  isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return predicted - target\n",
    "        loss = -target * np.log(predicted + 10 ** -100)\n",
    "        accuracy = Accuracy.multiClass(target,predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def hiddenError(target, predicted):\n",
    "        return target.T.dot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    op = []\n",
    "    bias = []\n",
    "    weight = []\n",
    "    output = []\n",
    "    isInput=False\n",
    "    noOfNodes = 0\n",
    "    activation = None\n",
    "    def __init__(self, inputSize=0, outputSize=0, activation=None,isInput=False,input=[]):\n",
    "        '''\n",
    "        inputSize -> no.of.input feature \n",
    "        outputSize -> no.of.output\n",
    "        '''\n",
    "        if isInput:\n",
    "            self.output = input\n",
    "            self.isInput = True\n",
    "            self.noOfNodes = input.shape[0]\n",
    "        else :\n",
    "            self.noOfNodes = inputSize\n",
    "            self.activation = activation\n",
    "            self.weight = np.random.rand(inputSize, outputSize) - 0.5\n",
    "            self.bias = np.random.rand(inputSize,1) - 0.5\n",
    "\n",
    "    def generateWeight(self,*r):\n",
    "        '''\n",
    "        Receive input as set that define the set shape\n",
    "        '''\n",
    "        return np.random.randn(*r) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    input = []\n",
    "    target = []\n",
    "    layers = []\n",
    "    history = {'loss': [], 'accuracy':[]}\n",
    "    loss = None\n",
    "\n",
    "    def __init__(self, input, target, loss):\n",
    "        self.loss = loss\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.layers.append(DenseLayer(None,None,None,isInput=True, input=self.input))\n",
    "\n",
    "    def append(self, node, activationFunction):\n",
    "        preNode = self.layers[len(self.layers) -1]\n",
    "        self.layers.append(DenseLayer(inputSize=node,outputSize=preNode.noOfNodes, activation=activationFunction))\n",
    "\n",
    "    def train(self, epoch=100, lr=0.01):\n",
    "        for i in range(epoch):\n",
    "            for j in range(len(self.layers)):\n",
    "                if not self.layers[j].isInput:\n",
    "                    self.layers[j].output = self.forWord(self.layers[j], self.layers[j-1].output)\n",
    "                if j == len(self.layers)-1 :\n",
    "                    loss = self.loss(self.target,self.layers[j].output)\n",
    "                    self.history['loss'].append(np.mean(loss['loss']))\n",
    "                    self.history['accuracy'].append(np.mean(loss['accuracy']))\n",
    "                    # index = len(self.history['loss']) -1\n",
    "                    print(f\"Epoch: {i+1} Loss: {self.history['loss'][-1]} Accuracy: {self.history['accuracy'][-1]}\")\n",
    "                    # calculate loss\n",
    "                    self.backPropogation(loss['loss'],lr)\n",
    "\n",
    "    def forWord(self, layer, input):\n",
    "        layer.op = np.dot(layer.weight, input) + layer.bias\n",
    "        return layer.activation(layer.op)\n",
    "\n",
    "    def backPropogation(self, loss,lr):\n",
    "        layer_length = len(self.layers)\n",
    "        for index, layer in enumerate(self.layers[::-1]):\n",
    "            if not layer.isInput:\n",
    "                pervious_node = self.layers[layer_length - index - 2]\n",
    "                oldw = np.copy(layer.weight)\n",
    "                layer.weight, layer.bias = self.gradient(\n",
    "                    layer.weight, layer.bias, pervious_node.output, loss,lr)\n",
    "                loss = Error.hiddenError(oldw, loss)\n",
    "\n",
    "    def gradient(self, w, b, x, err, lr=0.01):\n",
    "        w = w - (1/len(x[0]) * (err.dot(x.T))) * lr\n",
    "        b = b - (lr * np.mean(err, axis=1).reshape(b.shape))\n",
    "        return w,b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.30110137833617723 Accuracy: 0.13997619047619048\n",
      "Epoch: 2 Loss: 0.30675692296282164 Accuracy: 0.14007142857142857\n",
      "Epoch: 3 Loss: 0.31337219278343054 Accuracy: 0.13997619047619048\n",
      "Epoch: 4 Loss: 0.321234277926191 Accuracy: 0.14061904761904762\n",
      "Epoch: 5 Loss: 0.33071721975162965 Accuracy: 0.14188095238095239\n",
      "Epoch: 6 Loss: 0.34234776966772645 Accuracy: 0.14335714285714285\n",
      "Epoch: 7 Loss: 0.35690198711307564 Accuracy: 0.14516666666666667\n",
      "Epoch: 8 Loss: 0.3755298469555364 Accuracy: 0.14852380952380953\n",
      "Epoch: 9 Loss: 0.40002912687264597 Accuracy: 0.15178571428571427\n",
      "Epoch: 10 Loss: 0.4333196710591719 Accuracy: 0.15542857142857142\n",
      "Epoch: 11 Loss: 0.4804057098580758 Accuracy: 0.15814285714285714\n",
      "Epoch: 12 Loss: 0.5503249032936673 Accuracy: 0.16026190476190477\n",
      "Epoch: 13 Loss: 0.6607957078950193 Accuracy: 0.16030952380952382\n",
      "Epoch: 14 Loss: 0.8506420752288226 Accuracy: 0.15766666666666668\n",
      "Epoch: 15 Loss: 1.2212369631632314 Accuracy: 0.15045238095238095\n",
      "Epoch: 16 Loss: 2.120776689585209 Accuracy: 0.1332142857142857\n",
      "Epoch: 17 Loss: 5.43454646623088 Accuracy: 0.10442857142857143\n",
      "Epoch: 18 Loss: 13.88572351807052 Accuracy: 0.09035714285714286\n",
      "Epoch: 19 Loss: 20.93355169597037 Accuracy: 0.09035714285714286\n",
      "Epoch: 20 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 21 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 22 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 23 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 24 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 25 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 26 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 27 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 28 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 29 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 30 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 31 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 32 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 33 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 34 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 35 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 36 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 37 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 38 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 39 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 40 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 41 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 42 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 43 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 44 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 45 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 46 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 47 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 48 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 49 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 50 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 51 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 52 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 53 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 54 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 55 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 56 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 57 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 58 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 59 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 60 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 61 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 62 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 63 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 64 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 65 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 66 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 67 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 68 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 69 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 70 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 71 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 72 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 73 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 74 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 75 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 76 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 77 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 78 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 79 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 80 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 81 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 82 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 83 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 84 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 85 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 86 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 87 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 88 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 89 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 90 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 91 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 92 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 93 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 94 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 95 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 96 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 97 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 98 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 99 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n",
      "Epoch: 100 Loss: 20.945300828056556 Accuracy: 0.09035714285714286\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = NeuralNetwork(np.copy(x),np.copy(y),Error.crossEntropyLoss)\n",
    "\n",
    "model.append(10, Activation.ReLU)\n",
    "model.append(10,Activation.softmax)\n",
    "model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.10460289562838226 Accuracy: 0.0670952380952381\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,10) (784,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m, Activation\u001b[39m.\u001b[39mReLU)\n\u001b[1;32m      4\u001b[0m model1\u001b[39m.\u001b[39mappend(\u001b[39m10\u001b[39m,Activation\u001b[39m.\u001b[39msoftmax)\n\u001b[0;32m----> 5\u001b[0m model1\u001b[39m.\u001b[39;49mtrain(\u001b[39m10\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, epoch, lr)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m Accuracy: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# calculate loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackPropogation(loss[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m],lr)\n",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mNeuralNetwork.backPropogation\u001b[0;34m(self, loss, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m pervious_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[layer_length \u001b[39m-\u001b[39m index \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m     41\u001b[0m oldw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(layer\u001b[39m.\u001b[39mweight)\n\u001b[0;32m---> 42\u001b[0m layer\u001b[39m.\u001b[39mweight, layer\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(\n\u001b[1;32m     43\u001b[0m     layer\u001b[39m.\u001b[39;49mweight, layer\u001b[39m.\u001b[39;49mbias, pervious_node\u001b[39m.\u001b[39;49moutput, loss,lr)\n\u001b[1;32m     44\u001b[0m loss \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39mhiddenError(oldw, loss)\n",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m, in \u001b[0;36mNeuralNetwork.gradient\u001b[0;34m(self, w, b, x, err, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(\u001b[39mself\u001b[39m, w, b, x, err, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39;49m (\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39mlen\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]) \u001b[39m*\u001b[39;49m (err\u001b[39m.\u001b[39;49mdot(x\u001b[39m.\u001b[39;49mT))) \u001b[39m*\u001b[39;49m lr\n\u001b[1;32m     48\u001b[0m     b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m (lr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(err, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(b\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m w,b\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,10) (784,10) "
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetwork(np.copy(x),np.copy(y),Error.meanSquareError)\n",
    "\n",
    "model1.append(10, Activation.ReLU)\n",
    "model1.append(10,Activation.softmax)\n",
    "model1.train(10, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw = np.random.rand(10, 784) - 0.5\n",
    "hb = np.random.rand(10, 1) - 0.5\n",
    "ow = np.random.rand(10, 10) - 0.5\n",
    "ob = np.random.rand(10, 1) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forWord(x,w,b):\n",
    "    return np.dot(w, x) + b\n",
    "\n",
    "def backWord(err, w,b, x,lr = 0.01):\n",
    "    m = 1/len(x[0])\n",
    "    w = w - (m * (err.dot(x.T))) * lr\n",
    "    b = b - (m * np.sum(err)) * lr\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.3270952041136218, Accuracy : 0.14061904761904762\n",
      "Epoch : 2, Loss : 0.27718281627206987, Accuracy : 0.1515\n",
      "Epoch : 3, Loss : 0.2623601768536675, Accuracy : 0.15111904761904763\n",
      "Epoch : 4, Loss : 0.2535025849509646, Accuracy : 0.1523809523809524\n",
      "Epoch : 5, Loss : 0.24725942691165098, Accuracy : 0.15442857142857142\n",
      "Epoch : 6, Loss : 0.24252898914978216, Accuracy : 0.15619047619047619\n",
      "Epoch : 7, Loss : 0.2387655036969315, Accuracy : 0.15878571428571428\n",
      "Epoch : 8, Loss : 0.23566062444428917, Accuracy : 0.16164285714285714\n",
      "Epoch : 9, Loss : 0.23302554062189706, Accuracy : 0.16383333333333333\n",
      "Epoch : 10, Loss : 0.2307424954931994, Accuracy : 0.16723809523809524\n",
      "Epoch : 11, Loss : 0.2287188875058745, Accuracy : 0.17064285714285715\n",
      "Epoch : 12, Loss : 0.2268873924208291, Accuracy : 0.1745\n",
      "Epoch : 13, Loss : 0.22520577745815754, Accuracy : 0.1792857142857143\n",
      "Epoch : 14, Loss : 0.22364186886314358, Accuracy : 0.1858095238095238\n",
      "Epoch : 15, Loss : 0.22216967717984268, Accuracy : 0.1934047619047619\n",
      "Epoch : 16, Loss : 0.22077219505984375, Accuracy : 0.20216666666666666\n",
      "Epoch : 17, Loss : 0.21943140389236643, Accuracy : 0.211\n",
      "Epoch : 18, Loss : 0.21813176450755417, Accuracy : 0.22030952380952382\n",
      "Epoch : 19, Loss : 0.2168683760649332, Accuracy : 0.2298095238095238\n",
      "Epoch : 20, Loss : 0.21563682122383263, Accuracy : 0.2391904761904762\n",
      "Epoch : 21, Loss : 0.2144271149590764, Accuracy : 0.2489761904761905\n",
      "Epoch : 22, Loss : 0.2132338092747718, Accuracy : 0.2576904761904762\n",
      "Epoch : 23, Loss : 0.2120530662088599, Accuracy : 0.2636428571428571\n",
      "Epoch : 24, Loss : 0.2108837493100102, Accuracy : 0.26926190476190476\n",
      "Epoch : 25, Loss : 0.2097207089733368, Accuracy : 0.27385714285714285\n",
      "Epoch : 26, Loss : 0.20855943435373614, Accuracy : 0.27826190476190477\n",
      "Epoch : 27, Loss : 0.20739598290777805, Accuracy : 0.28245238095238095\n",
      "Epoch : 28, Loss : 0.20622824880096874, Accuracy : 0.2858095238095238\n",
      "Epoch : 29, Loss : 0.2050530048222437, Accuracy : 0.2892857142857143\n",
      "Epoch : 30, Loss : 0.20386861957136473, Accuracy : 0.29283333333333333\n",
      "Epoch : 31, Loss : 0.20267447599283425, Accuracy : 0.29697619047619045\n",
      "Epoch : 32, Loss : 0.20146761025185517, Accuracy : 0.3013571428571429\n",
      "Epoch : 33, Loss : 0.20024267952504335, Accuracy : 0.30538095238095236\n",
      "Epoch : 34, Loss : 0.1990013096446983, Accuracy : 0.3102142857142857\n",
      "Epoch : 35, Loss : 0.19774772409394192, Accuracy : 0.3152857142857143\n",
      "Epoch : 36, Loss : 0.19648093951171833, Accuracy : 0.32\n",
      "Epoch : 37, Loss : 0.1951974851252976, Accuracy : 0.32466666666666666\n",
      "Epoch : 38, Loss : 0.19389943733370626, Accuracy : 0.3296904761904762\n",
      "Epoch : 39, Loss : 0.19258631660359293, Accuracy : 0.3342857142857143\n",
      "Epoch : 40, Loss : 0.1912594866707608, Accuracy : 0.3395952380952381\n",
      "Epoch : 41, Loss : 0.18991826953794136, Accuracy : 0.3448095238095238\n",
      "Epoch : 42, Loss : 0.18856057621811245, Accuracy : 0.34983333333333333\n",
      "Epoch : 43, Loss : 0.18719044684886885, Accuracy : 0.35595238095238096\n",
      "Epoch : 44, Loss : 0.18580704077851395, Accuracy : 0.36304761904761906\n",
      "Epoch : 45, Loss : 0.18441492228916959, Accuracy : 0.37033333333333335\n",
      "Epoch : 46, Loss : 0.1830187824788439, Accuracy : 0.3774047619047619\n",
      "Epoch : 47, Loss : 0.18161892645962302, Accuracy : 0.383547619047619\n",
      "Epoch : 48, Loss : 0.18021547765884433, Accuracy : 0.39026190476190475\n",
      "Epoch : 49, Loss : 0.17881167098824724, Accuracy : 0.3959761904761905\n",
      "Epoch : 50, Loss : 0.17740889036941868, Accuracy : 0.402\n",
      "Epoch : 51, Loss : 0.17600779570837813, Accuracy : 0.4076190476190476\n",
      "Epoch : 52, Loss : 0.17460724246772408, Accuracy : 0.4127619047619048\n",
      "Epoch : 53, Loss : 0.17321292550498893, Accuracy : 0.41845238095238096\n",
      "Epoch : 54, Loss : 0.1718311536130277, Accuracy : 0.42292857142857143\n",
      "Epoch : 55, Loss : 0.17045819431632758, Accuracy : 0.42833333333333334\n",
      "Epoch : 56, Loss : 0.1690992452077623, Accuracy : 0.43338095238095237\n",
      "Epoch : 57, Loss : 0.1677571965476647, Accuracy : 0.43802380952380954\n",
      "Epoch : 58, Loss : 0.1664317209129723, Accuracy : 0.44266666666666665\n",
      "Epoch : 59, Loss : 0.16512350179096416, Accuracy : 0.4466428571428571\n",
      "Epoch : 60, Loss : 0.16382997288748985, Accuracy : 0.4509285714285714\n",
      "Epoch : 61, Loss : 0.16255474447702734, Accuracy : 0.4548809523809524\n",
      "Epoch : 62, Loss : 0.16130257164196615, Accuracy : 0.45814285714285713\n",
      "Epoch : 63, Loss : 0.16006980376637375, Accuracy : 0.4615714285714286\n",
      "Epoch : 64, Loss : 0.15885590905456834, Accuracy : 0.46473809523809523\n",
      "Epoch : 65, Loss : 0.15766159368228128, Accuracy : 0.4684047619047619\n",
      "Epoch : 66, Loss : 0.15648858907707636, Accuracy : 0.4718333333333333\n",
      "Epoch : 67, Loss : 0.1553408349109343, Accuracy : 0.47404761904761905\n",
      "Epoch : 68, Loss : 0.15421841986657744, Accuracy : 0.4767857142857143\n",
      "Epoch : 69, Loss : 0.1531187218916112, Accuracy : 0.4803095238095238\n",
      "Epoch : 70, Loss : 0.1520378366997807, Accuracy : 0.4830714285714286\n",
      "Epoch : 71, Loss : 0.15097594004793444, Accuracy : 0.4861904761904762\n",
      "Epoch : 72, Loss : 0.14993645719483115, Accuracy : 0.4895952380952381\n",
      "Epoch : 73, Loss : 0.1489189311767301, Accuracy : 0.49278571428571427\n",
      "Epoch : 74, Loss : 0.14792050349772354, Accuracy : 0.49633333333333335\n",
      "Epoch : 75, Loss : 0.14694230668593836, Accuracy : 0.49885714285714283\n",
      "Epoch : 76, Loss : 0.14598118145758956, Accuracy : 0.5014285714285714\n",
      "Epoch : 77, Loss : 0.1450359136768902, Accuracy : 0.5048809523809524\n",
      "Epoch : 78, Loss : 0.14410900920230946, Accuracy : 0.5076904761904761\n",
      "Epoch : 79, Loss : 0.14319706582577904, Accuracy : 0.5109285714285714\n",
      "Epoch : 80, Loss : 0.1422996892142719, Accuracy : 0.5139285714285714\n",
      "Epoch : 81, Loss : 0.14141690638423962, Accuracy : 0.516547619047619\n",
      "Epoch : 82, Loss : 0.14054745397160126, Accuracy : 0.5199285714285714\n",
      "Epoch : 83, Loss : 0.13969304451444006, Accuracy : 0.5228095238095238\n",
      "Epoch : 84, Loss : 0.13885091349361614, Accuracy : 0.5253095238095238\n",
      "Epoch : 85, Loss : 0.1380227027020987, Accuracy : 0.5275714285714286\n",
      "Epoch : 86, Loss : 0.13720643166416205, Accuracy : 0.5304285714285715\n",
      "Epoch : 87, Loss : 0.13639973101003935, Accuracy : 0.5341428571428571\n",
      "Epoch : 88, Loss : 0.13560504814129695, Accuracy : 0.5367619047619048\n",
      "Epoch : 89, Loss : 0.13482140156302969, Accuracy : 0.539547619047619\n",
      "Epoch : 90, Loss : 0.13404983835360287, Accuracy : 0.5421904761904762\n",
      "Epoch : 91, Loss : 0.13329037167489743, Accuracy : 0.5451190476190476\n",
      "Epoch : 92, Loss : 0.13254101245137703, Accuracy : 0.548404761904762\n",
      "Epoch : 93, Loss : 0.13180252571149714, Accuracy : 0.551404761904762\n",
      "Epoch : 94, Loss : 0.1310750092001862, Accuracy : 0.553952380952381\n",
      "Epoch : 95, Loss : 0.1303575726428521, Accuracy : 0.5563333333333333\n",
      "Epoch : 96, Loss : 0.12964799954965148, Accuracy : 0.5589761904761905\n",
      "Epoch : 97, Loss : 0.12894766078820907, Accuracy : 0.5617619047619048\n",
      "Epoch : 98, Loss : 0.12825696844893136, Accuracy : 0.5642380952380952\n",
      "Epoch : 99, Loss : 0.12757451008184517, Accuracy : 0.5668095238095238\n",
      "Epoch : 100, Loss : 0.12689993506287206, Accuracy : 0.5691428571428572\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    #forword\n",
    "    #Hidden\n",
    "    # hid_op = forWord(x,hw,hb)\n",
    "    hid_op = hw.dot(x)+hb\n",
    "    hid_act = Activation.ReLU(hid_op)\n",
    "\n",
    "    #op layer\n",
    "    op = ow.dot(hid_act)+ob\n",
    "    # op = forWord(hid_act,ow,ob)\n",
    "    y_pred = Activation.softmax(op)\n",
    "\n",
    "    #backword\n",
    "    #output error\n",
    "    op_err = y_pred - y\n",
    "    # ow,ob = backWord(op_err, ow, ob, hid_act)\n",
    "    ow = ow - (1/len(x[0]) * op_err.dot(hid_op.T)) * 0.1\n",
    "    ob = ob - (1/len(x[0]) * np.sum(op_err)) * 0.1\n",
    "\n",
    "    # #hidden error\n",
    "    hid_err = Error.hiddenError(ow,op_err) * Activation.ReLU(hid_op,True)\n",
    "    # hw,hb = backWord(hid_err, hw, hb, x)\n",
    "    hw = hw - (1/len(x[0]) * hid_err.dot(x.T)) * 0.1\n",
    "    hb = hb - (1/len(x[0]) * np.sum(hid_err)) * 0.1\n",
    "\n",
    "    loss = Error.crossEntropyLoss(y, y_pred)\n",
    "    print(f\"Epoch : {i + 1}, Loss : {np.mean(loss['loss'])}, Accuracy : {np.mean(loss['accuracy'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
