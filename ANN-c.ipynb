{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    '''\n",
    "    MNIST data set \n",
    "    x has 784 feature\n",
    "    y is op value from 0 to 9 \n",
    "    '''\n",
    "    data = np.array(pd.read_csv('data/MNIST/MNIST_train.csv'))\n",
    "    x = (data[:,1:]/255).T\n",
    "    y = oneHot(data[:,0])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def ReLU(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return Z > 0\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            op = Activation.sigmoid(Z)\n",
    "            return op * (1- op)\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z,isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 1\n",
    "        Z = Z - np.max(Z, axis=0)\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    @staticmethod\n",
    "    def multiClass(target, prediction):\n",
    "        return np.argmax(target, axis=0) == np.argmax(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error:\n",
    "    @staticmethod\n",
    "    def meanSquareError(target, predicted, isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return 2 * (predicted - target) / np.size(target)\n",
    "        loss = np.power(target - predicted, 2)\n",
    "        accuracy = Accuracy.multiClass(target, predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(target, predicted,  isDerivation=False):\n",
    "        if isDerivation:\n",
    "            return predicted - target\n",
    "        loss = -target * np.log(predicted + 10 ** -100)\n",
    "        accuracy = Accuracy.multiClass(target,predicted)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    @staticmethod\n",
    "    def hiddenError(target, predicted):\n",
    "        return target.T.dot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    op = []\n",
    "    bias = []\n",
    "    weight = []\n",
    "    output = []\n",
    "    isInput=False\n",
    "    noOfNodes = 0\n",
    "    activation = None\n",
    "    def __init__(self, inputSize=0, outputSize=0, activation=None,isInput=False,input=[]):\n",
    "        '''\n",
    "        inputSize -> no.of.input feature \n",
    "        outputSize -> no.of.output\n",
    "        '''\n",
    "        if isInput:\n",
    "            self.output = input\n",
    "            self.isInput = True\n",
    "            self.noOfNodes = input.shape[0]\n",
    "        else :\n",
    "            self.noOfNodes = inputSize\n",
    "            self.activation = activation\n",
    "            self.weight = self.generateWeight(inputSize, outputSize)\n",
    "            self.bias = self.generateWeight(inputSize,1)\n",
    "\n",
    "    def generateWeight(self,*r):\n",
    "        '''\n",
    "        Receive input as set that define the set shape\n",
    "        '''\n",
    "        return np.random.randn(*r) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    input = []\n",
    "    target = []\n",
    "    layers = []\n",
    "    history = {'loss': [], 'accuracy':[]}\n",
    "    loss = None\n",
    "\n",
    "    def __init__(self, input, target, loss):\n",
    "        self.loss = loss\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.layers.append(DenseLayer(None,None,None,isInput=True, input=self.input))\n",
    "\n",
    "    def append(self, node, activationFunction):\n",
    "        preNode = self.layers[len(self.layers) -1]\n",
    "        self.layers.append(DenseLayer(inputSize=node,outputSize=preNode.noOfNodes, activation=activationFunction))\n",
    "\n",
    "    def train(self, epoch=100):\n",
    "        for i in range(epoch):\n",
    "            for j in range(len(self.layers)):\n",
    "                if not self.layers[j].isInput:\n",
    "                    self.layers[j].output = self.forWord(self.layers[j], self.layers[j-1].output)\n",
    "                if j == len(self.layers)-1 :\n",
    "                    loss = self.loss(self.target,self.layers[j].output)\n",
    "                    self.history['loss'].append(np.mean(loss['loss']))\n",
    "                    self.history['accuracy'].append(np.mean(loss['accuracy']))\n",
    "                    index = len(self.history['loss']) -1\n",
    "                    print(f\"Epoch: {i+1} Loss: {self.history['loss'][index]} Accuracy: {self.history['accuracy'][index]}\")\n",
    "                    # calculate loss\n",
    "                    self.backPropogation(loss['loss'])\n",
    "\n",
    "    def forWord(self, layer, input):\n",
    "        layer.op = np.dot(layer.weight, input) + layer.bias\n",
    "        return layer.activation(layer.op)\n",
    "\n",
    "    def backPropogation(self, loss):\n",
    "        layer_length = len(self.layers)\n",
    "        for index, layer in enumerate(self.layers[::-1]):\n",
    "            if not layer.isInput:\n",
    "                pervious_node = self.layers[layer_length - index - 2]\n",
    "                oldw = np.copy(layer.weight)\n",
    "                layer.weight, layer.bias = self.gradient(\n",
    "                    layer.weight, layer.bias, pervious_node.output, loss)\n",
    "                loss = Error.hiddenError(oldw, loss)\n",
    "\n",
    "    def gradient(self, w, b, x, err, lr=0.01):\n",
    "        w = w - (1/len(x[0]) * (err.dot(x.T))) * lr\n",
    "        b = b - (lr * np.mean(err, axis=1).reshape(b.shape))\n",
    "        return w,b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.25886972097394667 Accuracy: 0.09035714285714286\n",
      "Epoch: 2 Loss: 0.2589264648076213 Accuracy: 0.09035714285714286\n",
      "Epoch: 3 Loss: 0.2589833115628361 Accuracy: 0.09035714285714286\n",
      "Epoch: 4 Loss: 0.25904026140484754 Accuracy: 0.09035714285714286\n",
      "Epoch: 5 Loss: 0.25909731449913165 Accuracy: 0.09035714285714286\n",
      "Epoch: 6 Loss: 0.2591547322981455 Accuracy: 0.09035714285714286\n",
      "Epoch: 7 Loss: 0.2592133360384557 Accuracy: 0.09035714285714286\n",
      "Epoch: 8 Loss: 0.2592723508148678 Accuracy: 0.09035714285714286\n",
      "Epoch: 9 Loss: 0.2593317281512697 Accuracy: 0.09035714285714286\n",
      "Epoch: 10 Loss: 0.2593914075033725 Accuracy: 0.09035714285714286\n",
      "Epoch: 11 Loss: 0.25945133499581047 Accuracy: 0.09035714285714286\n",
      "Epoch: 12 Loss: 0.25951145629250894 Accuracy: 0.09035714285714286\n",
      "Epoch: 13 Loss: 0.2595739599793103 Accuracy: 0.09038095238095238\n",
      "Epoch: 14 Loss: 0.25963885745529397 Accuracy: 0.09038095238095238\n",
      "Epoch: 15 Loss: 0.2597094302747388 Accuracy: 0.0904047619047619\n",
      "Epoch: 16 Loss: 0.25978954398867754 Accuracy: 0.0904047619047619\n",
      "Epoch: 17 Loss: 0.25988145854547123 Accuracy: 0.0904047619047619\n",
      "Epoch: 18 Loss: 0.2599826815918895 Accuracy: 0.0904047619047619\n",
      "Epoch: 19 Loss: 0.26010175413096026 Accuracy: 0.09042857142857143\n",
      "Epoch: 20 Loss: 0.2602442765049163 Accuracy: 0.09045238095238095\n",
      "Epoch: 21 Loss: 0.2604187308025477 Accuracy: 0.09045238095238095\n",
      "Epoch: 22 Loss: 0.26063141358351305 Accuracy: 0.09045238095238095\n",
      "Epoch: 23 Loss: 0.26089872620628696 Accuracy: 0.09045238095238095\n",
      "Epoch: 24 Loss: 0.2612415051125548 Accuracy: 0.09052380952380952\n",
      "Epoch: 25 Loss: 0.26168140127443484 Accuracy: 0.0904047619047619\n",
      "Epoch: 26 Loss: 0.26224109362664016 Accuracy: 0.09045238095238095\n",
      "Epoch: 27 Loss: 0.26296193901453085 Accuracy: 0.09033333333333333\n",
      "Epoch: 28 Loss: 0.26391618331307076 Accuracy: 0.09035714285714286\n",
      "Epoch: 29 Loss: 0.26519566722793 Accuracy: 0.09052380952380952\n",
      "Epoch: 30 Loss: 0.26692510355598115 Accuracy: 0.09052380952380952\n",
      "Epoch: 31 Loss: 0.26924822467229487 Accuracy: 0.09064285714285714\n",
      "Epoch: 32 Loss: 0.2723264907405072 Accuracy: 0.09059523809523809\n",
      "Epoch: 33 Loss: 0.27635754515005 Accuracy: 0.09057142857142857\n",
      "Epoch: 34 Loss: 0.28172444163906984 Accuracy: 0.09054761904761904\n",
      "Epoch: 35 Loss: 0.28890402891364636 Accuracy: 0.09083333333333334\n",
      "Epoch: 36 Loss: 0.2985684786539424 Accuracy: 0.09066666666666667\n",
      "Epoch: 37 Loss: 0.3115360046467645 Accuracy: 0.09080952380952381\n",
      "Epoch: 38 Loss: 0.3292617481309743 Accuracy: 0.09080952380952381\n",
      "Epoch: 39 Loss: 0.3540187738913617 Accuracy: 0.09085714285714286\n",
      "Epoch: 40 Loss: 0.3892095055181746 Accuracy: 0.09109523809523809\n",
      "Epoch: 41 Loss: 0.44109629021160723 Accuracy: 0.0916904761904762\n",
      "Epoch: 42 Loss: 0.5212348028307372 Accuracy: 0.0925\n",
      "Epoch: 43 Loss: 0.654423868858404 Accuracy: 0.09347619047619048\n",
      "Epoch: 44 Loss: 0.9007988904256596 Accuracy: 0.09542857142857143\n",
      "Epoch: 45 Loss: 1.4439297637955366 Accuracy: 0.10069047619047619\n",
      "Epoch: 46 Loss: 2.9216693440046435 Accuracy: 0.10857142857142857\n",
      "Epoch: 47 Loss: 6.675992737946226 Accuracy: 0.11933333333333333\n",
      "Epoch: 48 Loss: 18.930113216547095 Accuracy: 0.1000952380952381\n",
      "Epoch: 49 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 50 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 51 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 52 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 53 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 54 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 55 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 56 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 57 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 58 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 59 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 60 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 61 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 62 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 63 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 64 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 65 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 66 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 67 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 68 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 69 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 70 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 71 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 72 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 73 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 74 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 75 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 76 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 77 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 78 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 79 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 80 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 81 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 82 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 83 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 84 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 85 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 86 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 87 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 88 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 89 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 90 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 91 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 92 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 93 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 94 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 95 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 96 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 97 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 98 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 99 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n",
      "Epoch: 100 Loss: 20.735875231503282 Accuracy: 0.09945238095238096\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = NeuralNetwork(np.copy(x),np.copy(y),Error.crossEntropyLoss)\n",
    "\n",
    "model.append(10, Activation.ReLU)\n",
    "model.append(10,Activation.softmax)\n",
    "model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.09437236307824659 Accuracy: 0.09035714285714286\n",
      "Epoch: 2 Loss: 0.0943718741034192 Accuracy: 0.09035714285714286\n",
      "Epoch: 3 Loss: 0.09437138549594395 Accuracy: 0.09035714285714286\n",
      "Epoch: 4 Loss: 0.09437089725551835 Accuracy: 0.09035714285714286\n",
      "Epoch: 5 Loss: 0.09437040938184008 Accuracy: 0.09035714285714286\n",
      "Epoch: 6 Loss: 0.09436992187460727 Accuracy: 0.09035714285714286\n",
      "Epoch: 7 Loss: 0.09436943473351848 Accuracy: 0.09035714285714286\n",
      "Epoch: 8 Loss: 0.0943689479582724 Accuracy: 0.09035714285714286\n",
      "Epoch: 9 Loss: 0.09436846154856854 Accuracy: 0.09035714285714286\n",
      "Epoch: 10 Loss: 0.09436797550410629 Accuracy: 0.09035714285714286\n",
      "Epoch: 11 Loss: 0.09436748982458555 Accuracy: 0.09035714285714286\n",
      "Epoch: 12 Loss: 0.09436700450970671 Accuracy: 0.09035714285714286\n",
      "Epoch: 13 Loss: 0.09436651955917032 Accuracy: 0.09035714285714286\n",
      "Epoch: 14 Loss: 0.09436603497267743 Accuracy: 0.09035714285714286\n",
      "Epoch: 15 Loss: 0.09436555074992944 Accuracy: 0.09035714285714286\n",
      "Epoch: 16 Loss: 0.09436506689062796 Accuracy: 0.09035714285714286\n",
      "Epoch: 17 Loss: 0.09436458339447519 Accuracy: 0.09035714285714286\n",
      "Epoch: 18 Loss: 0.09436410026117348 Accuracy: 0.09035714285714286\n",
      "Epoch: 19 Loss: 0.0943636174904256 Accuracy: 0.09035714285714286\n",
      "Epoch: 20 Loss: 0.09436313508193467 Accuracy: 0.09035714285714286\n",
      "Epoch: 21 Loss: 0.09436265303540418 Accuracy: 0.09035714285714286\n",
      "Epoch: 22 Loss: 0.09436217135053801 Accuracy: 0.09035714285714286\n",
      "Epoch: 23 Loss: 0.09436169002704022 Accuracy: 0.09035714285714286\n",
      "Epoch: 24 Loss: 0.09436120906461547 Accuracy: 0.09035714285714286\n",
      "Epoch: 25 Loss: 0.09436072846296864 Accuracy: 0.09035714285714286\n",
      "Epoch: 26 Loss: 0.09436024822180482 Accuracy: 0.09035714285714286\n",
      "Epoch: 27 Loss: 0.09435976834082978 Accuracy: 0.09035714285714286\n",
      "Epoch: 28 Loss: 0.0943592888197493 Accuracy: 0.09035714285714286\n",
      "Epoch: 29 Loss: 0.09435880965826975 Accuracy: 0.09035714285714286\n",
      "Epoch: 30 Loss: 0.09435833085609777 Accuracy: 0.09035714285714286\n",
      "Epoch: 31 Loss: 0.09435785241294026 Accuracy: 0.09035714285714286\n",
      "Epoch: 32 Loss: 0.0943573743285046 Accuracy: 0.09035714285714286\n",
      "Epoch: 33 Loss: 0.09435689660249838 Accuracy: 0.09035714285714286\n",
      "Epoch: 34 Loss: 0.09435641923462972 Accuracy: 0.09035714285714286\n",
      "Epoch: 35 Loss: 0.09435594222460689 Accuracy: 0.09035714285714286\n",
      "Epoch: 36 Loss: 0.0943554655721387 Accuracy: 0.09035714285714286\n",
      "Epoch: 37 Loss: 0.09435488089853858 Accuracy: 0.09035714285714286\n",
      "Epoch: 38 Loss: 0.0943542872277035 Accuracy: 0.09035714285714286\n",
      "Epoch: 39 Loss: 0.09435369696208408 Accuracy: 0.09035714285714286\n",
      "Epoch: 40 Loss: 0.09435311139952562 Accuracy: 0.09035714285714286\n",
      "Epoch: 41 Loss: 0.09435253160383417 Accuracy: 0.09035714285714286\n",
      "Epoch: 42 Loss: 0.09435195836944951 Accuracy: 0.09035714285714286\n",
      "Epoch: 43 Loss: 0.09435139221680601 Accuracy: 0.09035714285714286\n",
      "Epoch: 44 Loss: 0.09435085354091248 Accuracy: 0.09035714285714286\n",
      "Epoch: 45 Loss: 0.09435037868449811 Accuracy: 0.09035714285714286\n",
      "Epoch: 46 Loss: 0.09434992305829705 Accuracy: 0.09035714285714286\n",
      "Epoch: 47 Loss: 0.0943495395268575 Accuracy: 0.09035714285714286\n",
      "Epoch: 48 Loss: 0.09434918953396748 Accuracy: 0.09035714285714286\n",
      "Epoch: 49 Loss: 0.09434885768892048 Accuracy: 0.09035714285714286\n",
      "Epoch: 50 Loss: 0.09434853779677961 Accuracy: 0.09035714285714286\n",
      "Epoch: 51 Loss: 0.09434807439355772 Accuracy: 0.09035714285714286\n",
      "Epoch: 52 Loss: 0.0943476084179616 Accuracy: 0.09035714285714286\n",
      "Epoch: 53 Loss: 0.09434710845058078 Accuracy: 0.09035714285714286\n",
      "Epoch: 54 Loss: 0.0943466827862114 Accuracy: 0.09035714285714286\n",
      "Epoch: 55 Loss: 0.09434632087971007 Accuracy: 0.09035714285714286\n",
      "Epoch: 56 Loss: 0.09434600044880352 Accuracy: 0.09035714285714286\n",
      "Epoch: 57 Loss: 0.09434548402199844 Accuracy: 0.09035714285714286\n",
      "Epoch: 58 Loss: 0.0943450002000017 Accuracy: 0.09035714285714286\n",
      "Epoch: 59 Loss: 0.09434459695993029 Accuracy: 0.09035714285714286\n",
      "Epoch: 60 Loss: 0.09434441392694728 Accuracy: 0.09035714285714286\n",
      "Epoch: 61 Loss: 0.09434440029224245 Accuracy: 0.09035714285714286\n",
      "Epoch: 62 Loss: 0.09434463775429007 Accuracy: 0.09035714285714286\n",
      "Epoch: 63 Loss: 0.09434514750328227 Accuracy: 0.09035714285714286\n",
      "Epoch: 64 Loss: 0.09434601743927333 Accuracy: 0.09035714285714286\n",
      "Epoch: 65 Loss: 0.09434733026044577 Accuracy: 0.09035714285714286\n",
      "Epoch: 66 Loss: 0.09434929591911535 Accuracy: 0.09035714285714286\n",
      "Epoch: 67 Loss: 0.09435145604669952 Accuracy: 0.09035714285714286\n",
      "Epoch: 68 Loss: 0.09435389133705598 Accuracy: 0.09035714285714286\n",
      "Epoch: 69 Loss: 0.09435705165527032 Accuracy: 0.09035714285714286\n",
      "Epoch: 70 Loss: 0.09436145582550798 Accuracy: 0.09035714285714286\n",
      "Epoch: 71 Loss: 0.09436709180271788 Accuracy: 0.09035714285714286\n",
      "Epoch: 72 Loss: 0.0943740128075346 Accuracy: 0.09035714285714286\n",
      "Epoch: 73 Loss: 0.0943822167158185 Accuracy: 0.09035714285714286\n",
      "Epoch: 74 Loss: 0.09439266910328349 Accuracy: 0.09035714285714286\n",
      "Epoch: 75 Loss: 0.09440541543957275 Accuracy: 0.09035714285714286\n",
      "Epoch: 76 Loss: 0.09442056221564195 Accuracy: 0.09035714285714286\n",
      "Epoch: 77 Loss: 0.09443784993773724 Accuracy: 0.09035714285714286\n",
      "Epoch: 78 Loss: 0.09445782490657943 Accuracy: 0.09035714285714286\n",
      "Epoch: 79 Loss: 0.09448042605215334 Accuracy: 0.09035714285714286\n",
      "Epoch: 80 Loss: 0.09450609639933427 Accuracy: 0.09035714285714286\n",
      "Epoch: 81 Loss: 0.0945351286641869 Accuracy: 0.09035714285714286\n",
      "Epoch: 82 Loss: 0.09456872730020621 Accuracy: 0.09033333333333333\n",
      "Epoch: 83 Loss: 0.09460715475128689 Accuracy: 0.09033333333333333\n",
      "Epoch: 84 Loss: 0.09465083220158238 Accuracy: 0.09033333333333333\n",
      "Epoch: 85 Loss: 0.09469986427256827 Accuracy: 0.09033333333333333\n",
      "Epoch: 86 Loss: 0.09475596770691318 Accuracy: 0.09033333333333333\n",
      "Epoch: 87 Loss: 0.09482140286629268 Accuracy: 0.09033333333333333\n",
      "Epoch: 88 Loss: 0.09489661464176506 Accuracy: 0.09033333333333333\n",
      "Epoch: 89 Loss: 0.09498136857433015 Accuracy: 0.09033333333333333\n",
      "Epoch: 90 Loss: 0.09507817184375904 Accuracy: 0.09033333333333333\n",
      "Epoch: 91 Loss: 0.09518737110017937 Accuracy: 0.09033333333333333\n",
      "Epoch: 92 Loss: 0.09530897358460669 Accuracy: 0.09033333333333333\n",
      "Epoch: 93 Loss: 0.09544328709163843 Accuracy: 0.09033333333333333\n",
      "Epoch: 94 Loss: 0.09559186575861661 Accuracy: 0.09033333333333333\n",
      "Epoch: 95 Loss: 0.09575604563633575 Accuracy: 0.09033333333333333\n",
      "Epoch: 96 Loss: 0.09593654036587154 Accuracy: 0.09035714285714286\n",
      "Epoch: 97 Loss: 0.09613515536383921 Accuracy: 0.09035714285714286\n",
      "Epoch: 98 Loss: 0.09635010992930265 Accuracy: 0.09035714285714286\n",
      "Epoch: 99 Loss: 0.0965849342436578 Accuracy: 0.09035714285714286\n",
      "Epoch: 100 Loss: 0.09683946108879378 Accuracy: 0.09038095238095238\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetwork(np.copy(x),np.copy(y),Error.meanSquareError)\n",
    "\n",
    "model1.append(10, Activation.ReLU)\n",
    "model1.append(10,Activation.softmax)\n",
    "model1.train(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
