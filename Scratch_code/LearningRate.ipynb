{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adagrad\n",
    "This varient of gradient is used to modify learning rate\n",
    "\n",
    "initial_learning_rate / sqrt(pre_learning_rate + square(gradient of weight))\n",
    "\n",
    "[Refer](https://www.youtube.com/watch?v=hgRyVZDU18Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_lr = 0.01\n",
    "lr = 0\n",
    "def fun(gradient):\n",
    "    global lr\n",
    "\n",
    "    lr = const_lr / math.sqrt((lr + (gradient * gradient)) + 0.001)\n",
    "    print(f\"Learning Rate {lr}, Weight {gradient}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.31622776601683794, Weight 0\n",
      "Learning Rate 0.00871303706786619, Weight 1\n",
      "Learning Rate 0.004993940385338747, Weight 2\n",
      "Learning Rate 0.0033322238984995283, Weight 3\n",
      "Learning Rate 0.0024996616137234937, Weight 4\n",
      "Learning Rate 0.0019998600282308946, Weight 5\n",
      "Learning Rate 0.0016665972298018812, Weight 6\n",
      "Learning Rate 0.0014285325584781799, Weight 7\n",
      "Learning Rate 0.0012499762845366586, Weight 8\n",
      "Learning Rate 0.001111095679496491, Weight 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fun(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0009999894446887268, Weight 10\n",
      "Learning Rate 0.0011110973940164006, Weight 9\n",
      "Learning Rate 0.0012499793843245301, Weight 8\n",
      "Learning Rate 0.0014285386311676173, Weight 7\n",
      "Learning Rate 0.0016666104533387212, Weight 6\n",
      "Learning Rate 0.001999893344114082, Weight 5\n",
      "Learning Rate 0.002499765666283984, Weight 4\n",
      "Learning Rate 0.003332685417536984, Weight 3\n",
      "Learning Rate 0.004997294269493072, Weight 2\n",
      "Learning Rate 0.009970147736353262, Weight 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fun(10-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0006666504151902141, Weight 15\n",
      "Learning Rate 0.0006666641975686576, Weight 15\n",
      "Learning Rate 0.0006666641975482393, Weight 15\n",
      "Learning Rate 0.0006666641975482395, Weight 15\n",
      "Learning Rate 0.0006666641975482395, Weight 15\n",
      "Learning Rate 0.001999933336765236, Weight 5\n",
      "Learning Rate 0.0016665972281051363, Weight 6\n",
      "Learning Rate 0.0014285325584782046, Weight 7\n",
      "Learning Rate 0.0012499762845366586, Weight 8\n",
      "Learning Rate 0.001111095679496491, Weight 9\n",
      "Learning Rate 0.001999915561520514, Weight 5\n",
      "Learning Rate 0.002499765664548737, Weight 4\n",
      "Learning Rate 0.0033326854175373052, Weight 3\n",
      "Learning Rate 0.004997294269493072, Weight 2\n",
      "Learning Rate 0.009970147736353262, Weight 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    # 1st five increase\n",
    "    # medium not change\n",
    "    # last five decrease\n",
    "    if i < 5:\n",
    "        fun(i+(15 - i))\n",
    "    elif i > 9:\n",
    "        fun(15 - i)\n",
    "    else:\n",
    "        fun(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from this what clearly visible is if weight increase learning rate decrease  \n",
    "\n",
    "When the weights are updates the learning rate will decrease. If a field value is sparse more num of 0 is present means it's weight is not update so no change in learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100  # Number of samples\n",
    "n = 2    # Number of features\n",
    "X = np.random.rand(m, n)\n",
    "y = 3 * X[:, 0] + 5 * X[:, 1] + 2 + 0.1 * np.random.randn(m)  # True coefficients: [3, 5], Intercept: 2\n",
    "x1 = X[:,0]\n",
    "x2 = X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.0032065075349251436, W2 : 0.003165004056550137, B : 0.0019163527573996433\n",
      "Epoch 0, Cost: 29.36993543245579\n",
      "W1 : 0.0032188002994112453, W2 : 0.0031766429628865686, B : 0.001923967040673669\n",
      "Epoch 1, Cost: 29.14046135639629\n",
      "W1 : 0.003231720978501566, W2 : 0.003188873780944287, B : 0.0019317095195350894\n",
      "Epoch 2, Cost: 28.911918082659973\n",
      "W1 : 0.0032447456471206334, W2 : 0.0032011990026778107, B : 0.0019395145462842692\n",
      "Epoch 3, Cost: 28.68428173637463\n",
      "W1 : 0.003257875654115021, W2 : 0.0032136198059211887, B : 0.0019473828839813379\n",
      "Epoch 4, Cost: 28.45755232205558\n",
      "W1 : 0.0032711122813894383, W2 : 0.00322613730582887, B : 0.001955315305814801\n",
      "Epoch 5, Cost: 28.231729840834934\n",
      "W1 : 0.0032844568317245606, W2 : 0.003238752634963862, B : 0.001963312597609259\n",
      "Epoch 6, Cost: 28.006814293863386\n",
      "W1 : 0.0032979106291878106, W2 : 0.0032514669436248807, B : 0.0019713755580844025\n",
      "Epoch 7, Cost: 27.782805682309984\n",
      "W1 : 0.003311475019569299, W2 : 0.0032642814001943923, B : 0.0019795049991204647\n",
      "Epoch 8, Cost: 27.55970400736255\n",
      "W1 : 0.0033251513708284957, W2 : 0.00327719719149488, B : 0.0019877017460302538\n",
      "Epoch 9, Cost: 27.33750927022803\n",
      "W1 : 0.0033389410735519673, W2 : 0.003290215523153536, B : 0.001995966637837953\n",
      "Epoch 10, Cost: 27.116221472132864\n",
      "W1 : 0.00335284554142246, W2 : 0.0033033376199756437, B : 0.0020043005275649024\n",
      "Epoch 11, Cost: 26.895840614323426\n",
      "W1 : 0.0033668662116996995, W2 : 0.00331656472632686, B : 0.002012704282522534\n",
      "Epoch 12, Cost: 26.676366698066385\n",
      "W1 : 0.003381004545713222, W2 : 0.0033298981065246827, B : 0.002021178784612707\n",
      "Epoch 13, Cost: 26.45779972464919\n",
      "W1 : 0.003395262029367597, W2 : 0.0033433390452393204, B : 0.002029724930635635\n",
      "Epoch 14, Cost: 26.240139695380375\n",
      "W1 : 0.003409640173660408, W2 : 0.0033568888479042595, B : 0.002038343632605644\n",
      "Epoch 15, Cost: 26.023386611590126\n",
      "W1 : 0.00342414051521337, W2 : 0.003370548841136791, B : 0.002047035818074986\n",
      "Epoch 16, Cost: 25.80754047463063\n",
      "W1 : 0.0034387646168169634, W2 : 0.003384320373168775, B : 0.0020558024304659552\n",
      "Epoch 17, Cost: 25.592601285876597\n",
      "W1 : 0.003453514067988993, W2 : 0.0033982048142879515, B : 0.0020646444294115537\n",
      "Epoch 18, Cost: 25.378569046725698\n",
      "W1 : 0.003468390485547496, W2 : 0.0034122035572900673, B : 0.002073562791104963\n",
      "Epoch 19, Cost: 25.16544375859904\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "non_zero_const = .0001\n",
    "b_lr = w2_lr = w1_lr = 0\n",
    "b = np.random.rand()\n",
    "w1 = np.random.rand()\n",
    "w2 = np.random.rand()\n",
    "\n",
    "# Gradient Descent or Batch Gradient Descent or Vanilla Gradient Descent\n",
    "for epoch in range(20):\n",
    "    y_p = ((x1* w1) + (x2 * w2)) +  b\n",
    "\n",
    "    err = y_p - y\n",
    "\n",
    "    w1_g = ((x1 * err).sum() / len(x1))\n",
    "    w2_g = ((x2 * err).sum() / len(x2)) \n",
    "    b_g  = (err.sum() / len(err))\n",
    "\n",
    "    w1_lr = lr / math.sqrt((w1_lr + (w1_g ** 2)) + non_zero_const)\n",
    "    w2_lr = lr / math.sqrt((w2_lr + (w2_g ** 2)) + non_zero_const)\n",
    "    b_lr = lr / math.sqrt((b_lr + (b_g ** 2)) + non_zero_const)\n",
    "\n",
    "\n",
    "    w1 -= w1_lr * w1_g \n",
    "    w2 -= w2_lr * w2_g\n",
    "    b -= b_lr * b_g\n",
    "\n",
    "    print(f'W1 : {w1_lr}, W2 : {w2_lr}, B : {b_lr}')\n",
    "    print(f'Epoch {epoch}, Cost: {np.mean(err ** 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons\n",
    "\n",
    "If more update on weight is more reduce in learning rate so the learning is very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_lr = 0.01\n",
    "lr = 0\n",
    "gama = 0.95\n",
    "def fun(gradient):\n",
    "    global lr\n",
    "\n",
    "    lr = const_lr / math.sqrt(((gama * lr) + (1 - gama)*(gradient * gradient)) + 0.001)\n",
    "    print(f\"Learning Rate {lr}, Weight {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.31622776601683794, Weight 0\n",
      "Learning Rate 0.01686898690376425, Weight 1\n",
      "Learning Rate 0.021465676484419955, Weight 2\n",
      "Learning Rate 0.01456494051327554, Weight 3\n",
      "Learning Rate 0.011078085459756751, Weight 4\n",
      "Learning Rate 0.00890332465956332, Weight 5\n",
      "Learning Rate 0.007434054274950055, Weight 6\n",
      "Learning Rate 0.006378279589596713, Weight 7\n",
      "Learning Rate 0.00558401402498321, Weight 8\n",
      "Learning Rate 0.004965176698345431, Weight 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fun(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.004469581461972115, Weight 10\n",
      "Learning Rate 0.00496582479018533, Weight 9\n",
      "Learning Rate 0.005585182566043016, Weight 8\n",
      "Learning Rate 0.00638055963694989, Weight 7\n",
      "Learning Rate 0.007438982372232794, Weight 6\n",
      "Learning Rate 0.008915549363344768, Weight 5\n",
      "Learning Rate 0.011114749578624333, Weight 4\n",
      "Learning Rate 0.014719273750746224, Weight 3\n",
      "Learning Rate 0.021567391769504722, Weight 2\n",
      "Learning Rate 0.037400750890082934, Weight 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fun(10-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0029765951249258707, Weight 15\n",
      "Learning Rate 0.0029809168908870756, Weight 15\n",
      "Learning Rate 0.0029809163471307292, Weight 15\n",
      "Learning Rate 0.0029809163471991437, Weight 15\n",
      "Learning Rate 0.002980916347199135, Weight 15\n",
      "Learning Rate 0.008930594032314757, Weight 5\n",
      "Learning Rate 0.007434001059066498, Weight 6\n",
      "Learning Rate 0.006378279655187909, Weight 7\n",
      "Learning Rate 0.0055840140249289625, Weight 8\n",
      "Learning Rate 0.004965176698345463, Weight 9\n",
      "Learning Rate 0.008923888335003297, Weight 5\n",
      "Learning Rate 0.011114695190764893, Weight 4\n",
      "Learning Rate 0.014719274574608771, Weight 3\n",
      "Learning Rate 0.02156739173024554, Weight 2\n",
      "Learning Rate 0.03740075089983903, Weight 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    # 1st five increase\n",
    "    # medium not change\n",
    "    # last five decrease\n",
    "    if i < 5:\n",
    "        fun(i+(15 - i))\n",
    "    elif i > 9:\n",
    "        fun(15 - i)\n",
    "    else:\n",
    "        fun(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.01399185503272005, W2 : 0.013763235520220598, B : 0.008411495754837035\n",
      "Epoch 0, Cost: 30.842483368541647\n",
      "W1 : 0.014051301196143434, W2 : 0.013820015218616491, B : 0.00853656833602826\n",
      "Epoch 1, Cost: 29.799645540861974\n",
      "W1 : 0.014294593258881823, W2 : 0.014048810342239357, B : 0.008689012658219171\n",
      "Epoch 2, Cost: 28.78345690810883\n",
      "W1 : 0.014543525247786303, W2 : 0.014282710195099869, B : 0.008846823761023245\n",
      "Epoch 3, Cost: 27.78544586694567\n",
      "W1 : 0.014800798833435307, W2 : 0.01452411097554255, B : 0.009010340134765659\n",
      "Epoch 4, Cost: 26.80571011759724\n",
      "W1 : 0.015066766794344802, W2 : 0.014773317615488191, B : 0.009179866849231938\n",
      "Epoch 5, Cost: 25.844254802610426\n",
      "W1 : 0.01534183503180607, W2 : 0.015030679608639264, B : 0.009355730146949499\n",
      "Epoch 6, Cost: 24.901086572950415\n",
      "W1 : 0.01562643035177168, W2 : 0.015296563818606697, B : 0.009538279207942493\n",
      "Epoch 7, Cost: 23.97621242203656\n",
      "W1 : 0.0159210018304856, W2 : 0.015571355587206005, B : 0.009727888063596772\n",
      "Epoch 8, Cost: 23.069639719028533\n",
      "W1 : 0.0162260216380353, W2 : 0.015855459417633822, B : 0.009924957688482347\n",
      "Epoch 9, Cost: 22.18137622556021\n",
      "W1 : 0.016541985787793388, W2 : 0.01614929960771426, B : 0.010129918282890286\n",
      "Epoch 10, Cost: 21.31143011207109\n",
      "W1 : 0.01686941476829396, W2 : 0.016453320802523888, B : 0.010343231762239396\n",
      "Epoch 11, Cost: 20.45980997302572\n",
      "W1 : 0.017208854013079273, W2 : 0.016767988435551637, B : 0.010565394469961688\n",
      "Epoch 12, Cost: 19.62652484041994\n",
      "W1 : 0.01756087415338109, W2 : 0.01709378902030997, B : 0.010796940130630303\n",
      "Epoch 13, Cost: 18.81158419480412\n",
      "W1 : 0.017926070985873222, W2 : 0.01743123024577505, B : 0.011038443059730427\n",
      "Epoch 14, Cost: 18.014997972849336\n",
      "W1 : 0.018305065072701514, W2 : 0.017780840818891485, B : 0.01129052164536712\n",
      "Epoch 15, Cost: 17.23677657022973\n",
      "W1 : 0.018698500873243186, W2 : 0.018143169985407197, B : 0.011553842115048698\n",
      "Epoch 16, Cost: 16.4769308382829\n",
      "W1 : 0.01910704528625848, W2 : 0.01851878664628067, B : 0.011829122597073789\n",
      "Epoch 17, Cost: 15.73547207252747\n",
      "W1 : 0.0195313854570135, W2 : 0.018908277970617587, B : 0.012117137480442922\n",
      "Epoch 18, Cost: 15.012411990647173\n",
      "W1 : 0.01997222567641719, W2 : 0.01931224738738176, B : 0.012418722068895856\n",
      "Epoch 19, Cost: 14.307762696977832\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "gama = 0.95\n",
    "non_zero_const = .0001\n",
    "b_lr = w2_lr = w1_lr = 0\n",
    "b = np.random.rand()\n",
    "w1 = np.random.rand()\n",
    "w2 = np.random.rand()\n",
    "\n",
    "# Gradient Descent or Batch Gradient Descent or Vanilla Gradient Descent\n",
    "for epoch in range(20):\n",
    "    y_p = ((x1* w1) + (x2 * w2)) +  b\n",
    "\n",
    "    err = y_p - y\n",
    "\n",
    "    w1_g = ((x1 * err).sum() / len(x1))\n",
    "    w2_g = ((x2 * err).sum() / len(x2)) \n",
    "    b_g  = (err.sum() / len(err))\n",
    "\n",
    "    w1_lr = lr / math.sqrt(((gama * w1_lr) + ((1-gama) *(w1_g ** 2))) + non_zero_const)\n",
    "    w2_lr = lr / math.sqrt(((gama * w2_lr) + ((1 - gama)*(w2_g ** 2))) + non_zero_const)\n",
    "    b_lr = lr / math.sqrt(((gama * b_lr) + ((1 - gama)*(b_g ** 2))) + non_zero_const)\n",
    "\n",
    "\n",
    "    w1 -= w1_lr * w1_g \n",
    "    w2 -= w2_lr * w2_g\n",
    "    b -= b_lr * b_g\n",
    "\n",
    "    print(f'W1 : {w1_lr}, W2 : {w2_lr}, B : {b_lr}')\n",
    "    print(f'Epoch {epoch}, Cost: {np.mean(err ** 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
