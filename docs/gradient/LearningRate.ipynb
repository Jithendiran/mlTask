{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "Until now the learning rate $(\\alpha)$ is same for all iteration and same learning rate for all the input x vales (x1,x2,x3,...)\n",
    "if some feature has sparse vector( most of thr values are 0 only few values have 1) and if the learning rate is constant means for the sparse vector column the weights will not update in most of the case\n",
    "\n",
    "if x1 = [1,0,0,1,0,0,0,0,0,1], x2 = [1,0.8,34,382,....] and loss function is MSE and it's derivation is $2*x*error$\n",
    "\n",
    "x1 is sparse and x2 is dense\n",
    "\n",
    "Our weight updation fourmula is $wx1=wx1-learningRate * \\nabla x1$ if x1 is 0 means gradient also 0 so the weight updation will not  happen $wx1=wx1-learningRate * (2 * 0 * err) = wx1 - 0 = wx1$\n",
    "\n",
    "Our weight updation fourmula is $wx2=wx2-learningRate * \\nabla x2$ for x2 the values are dense vector so weight updation will happen all the times. for dense column weights are updating fast $wx2=wx2-learningRate * (2 * some\\_value * err) = wx1 - some\\_value = change\\_in\\_weight$\n",
    "\n",
    "Due to the issue of sparse vetcor some feature has slow learning. To solve this issue Adagrad will calculate different learning rate for different feature (x1, x2, x3,...)\n",
    "\n",
    "Adagrad will reduce the learning rate if previous weight is updated\n",
    "\n",
    "In adagrad if a feature is updating means reduce the learning rate, else maintain the learning rate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
